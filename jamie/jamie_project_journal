Jamie Davis <davisjam@vt.edu>
22 October 2015

I went into src/threadpool.c and added printfs to 
 work
  > this actually performs a work item
    calls w->work
 uv__work_done
  > invokes the 'done' callback on a completed work item
    calls w->done

 include/uv-threadpool.h defines the uv__work item, with
 the work and done fields defined as:

   void (*work)(struct uv__work *w);
   void (*done)(struct uv__work *w, int status);

FORWARD ORDER
1
stat hello 0x1897380
stat world 0x1897aa0
rename 0x1896a40

2
stat hello 0x19cb380
rename 0x19caa40
stat world 0x19cbaa0

3
stat hello 0x1539380
stat world 0x1539aa0
rename 0x1538a40

4
stat hello
rename
stat world

5
stat hello
stat world
rename

6
stat hello
stat world
rename

7
stat hello
stat world
rename

8
stat hello
stat world
rename

REVERSED ORDER
1
rename 0x13c8848
stat world 0x1663aa0
stat hello 0x1663380

2
rename 0x2873a40
stat world 0x2874aa0
stat hello 0x2874380

3
rename
stat world
stat hello

4
rename
stat world
stat hello

5
stat world
rename
stat hello

------------------

Conclusion:
  0. file_system.js is a useful tool to explore node+libuv interactions.

  1. The order in which callbacks enter the queue in uv__work_done
    depends on the order in which the underlying FS operations complete.

  2. Once they reach the queue, we can re-order them, as demonstrated.
    However it's the thread pool ordering that determines the order 
    in which they reach the queue.
    I haven't yet identified a way to determine which callback is which
    across runs (i.e. which one is 'stat' vs 'rename' in run X?).
    This would allow me to re-order function and callback invocations.
    
  3. To do that, I think I need to know how 'work' and 'done' get set
    for each `struct uv__work'.

  4. Just because a 'stat' command succeeded does not mean that the 
     stat'd file still exists at the time of the callback.

  5. Can re-order things all we want (both threadpool execution order 
      and global callback order) in src/threadpool.c, PROVIDED
      we can identify what we're working with.

  6. The libuv forum suggests that the threadpool is only used to
      emulate asynchronous-less synchronous operations like AIO.
      Where are asynchronous-native operations like sockets handled?

Open questions:
  1. Where are asynchronous-native operations like sockets handled?
  2. How does node.js submit operation+callback requests to libuv?
      >> $NODE/src/*wrap.cc ?
  3. Can we add user-defined data to libuv structs for label propagation?

---------------------------------

Jamie Davis <davisjam@vt.edu>
23 October 2015

Yesterday I discussed the open questions above with Dr. Lee.
#1: He suggested that finding this out would be valuable.
#2, #3: This may be irrelevant. If a fork (and-wait?) approach is used,
  the parent can launch a child to explore the path not taken.
  In such a setting, there's no need to know WHAT the work payload is,
  because the memory address itself can be used as the ID.
  I was thinking that we had to propagate labels in order to identify items
  between runs, but if I stick to a PARTICULAR run then I can use fork to
  reorder arbitrarily. 
  
  See test_code/fork_test.c and test_code/fork_test.txt for some sample code 
    along these lines.

  New open questions:
  1. Where are asynchronous-native operations like sockets handled?
     Is every callback executed in uv__work_done?
      > Explore http server, since socket code is not done by threadpool.
        I believe the answer is yes.
  2. Play with fork!

-------------------------

24 October 2015

Addressing the open questions from 23 October:

1. Regarding asynchronous-native operations like sockets:
   The callback is executed in uv__io_poll.
   Testing with 'node simple_http':
     uv__io_poll: calling w 0x2bd84a8's cb 0xdbff40 with {events 1, data 13}
     APP: Server handled a client!
     uv__io_poll: finished calling w 0x2bd84a8's cb 0xdbff40 with {events 1, data 18446744073709551615}

   The callback function is:
     gdb `which node`
     info symbol 0xdbff40
     uv.stream_io in section .text

     src/unix/stream.c:

   Uncomfortably, multiple callbacks can be pending in each phase of the loop, and furthermore
     in uv__io_poll we may call a single callback that routes us into threadpool: uv__work_done
     which calls multiple callbacks.
     It's not clear how to change the order of these callbacks at the level of uv__io_poll.

   Suppose that for starters we only re-order callbacks in threadpool::worker and threadpool::uv__work_done.
   This should be enough to see some interesting behavior.

2. Playing with fork: 
   I modified uv__work_done as follows:
    - wait until 4 callbacks show up (not sure what they all are)
    - shmem
    - fork()
      > child goes first and runs callbacks in forward order
      > parent sees child is done, goes second, and runs callbacks in reverse order
    Result: success, callbacks are executed in forward and reverse order as expected.

   This raises new questions.

    1. Unlike race conditions in memory, in node the race conditions can and do modify external
       resources like the file system or databases. 

       Different copies must not be allowed to see each other's actions. This suggests the need
       to ``reset'' things to the previous state. In the filesystem, taking a snapshot/rsync-backup
       should suffice (provided it preserves things like atime "just in case"). The database presumably
       also live in the filesystem. Network traffic can't be rolled back, though, so if network traffic
       is involved then the recipient must be able to "roll back" as well.

       This may be a limitation on the types of Node.js applications we can test. 
       If we treat Node.js applications as a "black box" then we need a way to reset the entire system.

    2. Need a way to decide when we have enough pending callbacks to fork and execute.
       Perhaps every time a new callback is received we can fork () and either start executing or NOT start executing.

    3. Review papers on reducing the exponential search space, e.g. partial order reduction.

    4. What happens if one process errors out? I'm guessing it closes stdout and stderr, discomfiting us. Not sure though.
       Need to introduce a fatal error into file_system.js under a particular interleaving.

-------------------------

26 October 2015

4. above: Regarding error'ing out: 
  If one process exits, the other's output is unaffected. It must dup the file descriptors. In retrospect, this makes sense.
  However, if one process relies on the other to log its completion, then we're in trouble.
  Need to wait for completion OR exit. I added a working implementation of this in threadpool.c.

-------------------------

27 October 2015

Discussion with Dr. Lee:
  - Bounded waiting: in time and in number of items in the queue
  - Flip order of only the first X items in the queue
  - Can't change order of JUST execution (cb->work) or JUST cleanup (cb->done),
      because execution maps to the 'request time' and cleanup maps to the 'done time'.
      Two work items could be done in order A, B, and finished in order B, A.
  - Partial order reduction: 
    > Need to read these papers (static [80s], dynamic [00s])
    > Only explore interleavings that actually affect each other
    > Issue here -- how to know if they affect each other in the "black box" approach to callbacks?
      Callbacks X and Y may not be directly related (surface level), but X may trigger callback Z that will affect Y.

  - Bounded reordering: added env variable REORDER_DISTANCE. If 0, do nothing. Otherwise use that reorder distance. Default is 0.
    > Haven't implemented the behavior yet.   
    > One paper used 'forward/backward' rather than spawning K! children. This sounds like a good move.
  - Wrote min_reordering.js that will exit(0) if reordering < X, otherwise exit(1).
    min_reordering.js uses dynamically-generated code so that I don't have to eyeball addresses for correctness;
    each incrementer function has its own "name". Hooray for closures!
  - Developed doubly linked list for a much-cleaner-to-work-with done_list.

-------------------------

28 October 2015

Implemented bounded waiting mechanism based on timeout and REORDER_DISTANCE.
Not quite working -- not quite understanding how to trigger epoll_wait to go back to uv__work_done.
  > output manipulation
  > log function that prefixes pid
  > figure out why epoll_wait isn't being nice

-------------------------

29 October 2015

Still working on the epoll_wait issue.
  - Added 'generation' to mylog for indentation. This should help clarify issues.
  - Idea: what if we spawn ALL children at a level before proceeding on one? This might help
      if the issue is some weird conflict between epoll and fork () (the internet suggests that
      there might be one).

      This leads to memory overhead (a weird combo of BFS and DFS search, but still painful), but 
      if it fixes the problem them that would be swell.

-------------------------

30 October 2015

- Used the combination DFS-BFS search as proposed on 29 Oct. This fixes the problem, hooray!

TODO (by order of priority):
  1. > Need a way to "prove" that it's working properly on a wide variety of inputs.
    Can generate simulated output for min_reordering.js and emit that, then compare.

  2. > Need to determine whether or not child exited 0.

  3. > Need to re-think the notion of a timeout. Because parents wait for children recursively,
    using clock time is a bad idea. 

    Propose instead using "number of loops since I last ran a callback". 
    This would require changing all callback calls to use a uv-common function that sets a 
    flag and then calls the callback. Then in main loop we could bump a counter, also through
    uv-common

  4. > Must avoid "wait forever"-style epoll requests. Never return -1 from the timout function.
    Instead, add a sleep in the main loop -- this effectively simulates "forever", although
    if done naively this would cause "idle" callbacks (and others?) to be executed many more 
    times than might be expected. Might need to take more care with this change.

    3,4: If each loop waits for a second, then "loops since I last ran a callback" has a lower bound of "seconds since I last ran a callback"

-------------------------

8 Nov 2015

- Fixed bug in list_size. Code is now in working order, ready to proceed on verifying correctness tomorrow.

-------------------------

9 Nov 2015

Addressing 1. from 30 October:
  1. Wrote simulator (min_reordering_exploration.pl) to simulate min_reordering.js's output
     when run under a modified Node. Initial results suggest that the simulator output matches
     the real output. Hooray!

  2. Implemented the node-mocks guy's fs.stat example to show the more general power of the 
     libuv reordering approach.

Addressing 2. from 30 October:
  1. Added exit status checking.

  2. It appears that fork() -> pthread_join() is a bad combination. See for example
      developerweb.net/viewtopic.php?id=3633
     Since this is a bit orthogonal, I just don't clean up the thread pool threads.

Outstanding TODOs, carried over from 30 October:
  1. Need to re-think the notion of a timeout. Because parents wait for children recursively,
    using clock time is a bad idea. 

    Propose instead using "number of loops since I last ran a callback". 
    This would require changing all callback calls to use a uv-common function that sets a 
    flag and then calls the callback. Then in main loop we could bump a counter, also through
    uv-common

  2. > Must avoid "wait forever"-style epoll requests. Never return -1 from the timout function.
    Instead, add a sleep in the main loop -- this effectively simulates "forever", although
    if done naively this would cause "idle" callbacks (and others?) to be executed many more 
    times than might be expected. Might need to take more care with this change.

    3,4: If each loop waits for a second, then "loops since I last ran a callback" has a lower bound of "seconds since I last ran a callback"

-------------------------

13 Nov 2015
    
Discussed progress with Dongyoon. Many ideas on paper.

-------------------------

16 Nov 2015

1. Question: How are fs.X and fs.XSync implemented?
  Answer: X is routed into libuv's thread pool work queue for asynchronous handling. XSync () is routed into libuv where it is called synchronously in uv__fs_work. The result is passed back up.

  See the '16 Nov 2015' entry in jamie_node_src_notes for details.

2. I reviewed issues with fork and pthreads. Most notably, in the child only the thread that
called fork() survives. I verified this using the fork_pthread.c test. 

Happily, Node.js only uses two sets of threads: the event loop thread (1 thread) and 
the thread pool threads (X threads).
As a result, getting to a coherent state prior to fork and re-initializing threads after fork
should not be difficult with the judicious use of locks.
  http://www.linuxprogrammingblog.com/threads-and-fork-think-twice-before-using-them
  http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_atfork.html

This probably also explains the segfault in pthread_join. Note that
both v8 and libuv make use of pthread_join, suggesting that we need
a pthread_at_fork call in both layers.

Question: When and why does v8's Thread::Start get called? I have added prints to show that it happens,
but have not yet tracked down WHY it happens. Next step: gdb

  cdprog
  gdb `which node`

  (gdb) break v8::base::Thread::Start
  Breakpoint 1 at 0xdfa174
  (gdb) c
  The program is not being run.
  (gdb) run file_system.js 
  Starting program: /home/jamie/bin/node file_system.js
  Traceback (most recent call last):
    File "/usr/share/gdb/auto-load/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.19-gdb.py", line 63, in <module>
      from libstdcxx.v6.printers import register_libstdcxx_printers
  ImportError: No module named 'libstdcxx'
  [Thread debugging using libthread_db enabled]
  Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

  Breakpoint 1, 0x0000000000dfa174 in v8::base::Thread::Start() ()
  (gdb) bt
  #0  0x0000000000dfa174 in v8::base::Thread::Start() ()
  #1  0x0000000000dcb9bc in v8::platform::WorkerThread::WorkerThread(v8::platform::TaskQueue*) ()
  #2  0x0000000000dc9191 in v8::platform::CreateDefaultPlatform(int) ()
  #3  0x0000000000d68e9e in node::Start(int, char**) ()
  #4  0x00007ffff6becec5 in __libc_start_main (main=0x688310 <main>, argc=2, argv=0x7fffffffe508, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe4f8)
        at libc-start.c:287
  #5  0x000000000068852f in _start ()
  (gdb)

Answer: The threads are launched as part of default_platform, whatever that is.

Conclusion:
pthread_atfork needed to be added at each layer that calls pthreads.
Two places:
  1. V8 uses a pool of threads for its default_platform (whatever that is)
    At the V8 layer, these threads seem to be used to provide "isolates" (independent interpreters?), and it doesn't look like Node makes use of "isolates".
    As a result, no synchronization seemed to be required. I just added a Reinitialize function that would empty and re-create the thread pool.
    
  2. libuv uses a pool of threads for its asynchronous worker pool
    These worker threads are all active, and an uncontrolled fork() could result in inconsistent memory states,
      locked-never-to-be-released mutexes, etc.
    To resolve this, I added a posix semaphore to control the number of active threads. 
    By down'ing this semaphore nthreads times and waiting for all threads to be waiting, I can ensure that
      all worker threads are in a quiesced state at the time of fork ().
      On resume: 
        - the parent post's to the sema
        - the child re-initializes all of the pthread variables and launches nthreads threads

-------------------------

19 Nov 2015

1. Following up on the pthread question, I put printfs into the code to figure out the path by which
the v8 pthreads are assigned work. Work gets into their shared queue via DefaultPlatform::CallOnBackgroundThread

CallOnBackgroundThread seems to be used for compilation and garbage collection tasks
(grep'd the full tree including node and v8). It is called here:

v8/src/optimizing-compile-dispatcher.cc

  OptimizingCompileDispatcher::QueueForOptimization calls:
      V8::GetCurrentPlatform()->CallOnBackgroundThread(
              new CompileTask(isolate_), v8::Platform::kShortRunningTask);


  OptimizingCompileDispatcher::Unblock() calls:
      V8::GetCurrentPlatform()->CallOnBackgroundThread(
              new CompileTask(isolate_), v8::Platform::kShortRunningTask);
  
v8/src/heap/mark-compact.cc
  MarkCompactCollector::StartSweeperThreads calls:
    V8::GetCurrentPlatform()->CallOnBackgroundThread(
          new SweeperTask(heap(), heap()->old_space()),
                v8::Platform::kShortRunningTask);

The sample cluster.js program is sufficient to exercise these threads.
This means that I DEFINITELY need to correctly re-initialize them in a controlled fashion.

Since the WorkerThreads end up waiting on a semaphore in TaskQueue::GetNext, there's already
a built-in waiting mechanism. I added a counter to track the number of waiters, and taught
DefaultPlatform to wait for the queue to empty and all the waiters to be pending prior to
fork'ing. Since the tasks seem to be short (kShortRunningTask, whatever that means!), 
I expect that if the main thread pauses to wait for it to empty, then it will empty fairly quickly.

Implementation appears to have worked. I added a few functions to TaskQueue and now
DefaultPlatform asks it if all of its threads are waiting on it. Since no OTHER threads should be
waiting on the DefaultPlatform's TaskQueue (right?), this seems fine.

2. The Node.js cluster module uses fork(). This makes it unsuitable
to use in my testing environment, since I fork() too. The parent does not know what pid to wait on, and there's
no (?) way to tell the parent that it has a NEW child to wait on (and you'd need to fork the parent itself
to wait on the new child every time the child forks...). Anyway it just seems like a mess.

3. I read the microarchitecture paper on Node.js (I-cache performance issues). It reminded me that as a largely-server-side programming framework, we'll need to think about network inputs as we fork. If we run through once to completion, we can record the network inputs. Afterward we can simulate them? Hmm...

4. I took a peek at how callbacks are invoked:

jamie@lqingrui:~/Desktop/node_project/node/deps/uv/src$ grep -Ri cb\( . | wc -l
141

Threadpool callbacks take 1 or 2 args:
  jamie@lqingrui:~/Desktop/node_project/node/deps/uv/src$ grep -Ri cb\( . | grep threadpool
  ./threadpool.c:  req->work_cb(req);
  ./threadpool.c:  req->after_work_cb(req, err);


More general callbacks take between 1 and 5 args. Here are examples of each:
  ./unix/fs.c:  req->cb(req);
  ./unix/udp.c:      req->send_cb(req, req->status);
  ./unix/getaddrinfo.c:    req->cb(req, req->retcode, req->addrinfo);
  ./unix/getnameinfo.c:    req->getnameinfo_cb(req, req->retcode, host, service);
  ./win/udp.c:      handle->recv_cb(handle, UV_ENOBUFS, &buf, NULL, 0);

19 Nov 2015
Meeting with Dr. Lee.

1. He has a literature list, to which I am welcome to add. For the next meeting, read the Bounded POR and DPOR papers.
2. Discussion about dealing with network traffic. I assert that the major use case for Node.js is as a server to fulfill network requests.  Consequently it's important to be able to handle this.  Probably not an open system (maintaining connections tricky), but can make a closed system (feed the same input). This requires that the client's input does not change based on the server's output. There are clearly cases where this is not true (e.g. two concurrent requests, one to delete a file and one to stat it, and the output will be either "no such file" and "stat values" -- the client will respond differently based on the server's decisions. So let's not discard an open system forever, but for now it's probably necessary to get something working.

3. Proposed architecture:
A. Fork preserves memory state
B. Use file system checkpointing to roll back changes from one run to the next
C. Record network inputs (connections, client data, etc.) and feed that into subsequent runs. Can record the loop number so that we can feed it at the appropriate spot. Modify the epoll mechanism to replay network input at the appropriate spot instead of looking for a live connection. Children close the parent's connections.

4. Question: Node.js uses a lot of external modules. Are these pure JS? Do they interact with libuv? Do they run JS code in the thread pool "work" or just as "done" (e.g. database requests/callbacks)? (single-threaded vs. multi-threaded event driven)

5. Plan to attend ASPLOS Apr. 2-6 '16 in Atlanta.

-----------

15 Dec 2015

1.
I've implemented a unified callback mechanism in an attempt to avoid the issues related to fork.
jamie@suwarna7-Lenovo-K450e:~/Desktop/node_project/node/deps/uv$ git branch
  master
* unified_callback
  uv_reordering

I construct a tree of dependencies between callbacks based on the currently-executing CB.
I'm working on verifying the result. The graphviz library and the xdot viewer for it look useful.

-----------

16 Dec 2015

1. I added a bit more information to a 'struct callback_node'. Now on USR2, the trees are printed 
in graphviz's DOT language for easy visualization. Looks good.

2. The tree looks OK but there's no information about cause and effect. 
In the MUD Node.js project, when a client makes a move it triggers messages to the other clients.
However, the subsequent messages are not children of the client CB.
If I can figure out when the fds of the client CBs are triggered, I can include the info about "who tickled me"
and expand my "cause and effect" picture.

-----------

17 Dec 2015

1. I set the WORK_WORK CBN as the parent of the subsequent WORK_DONE CBN.
2. Now dumping all trees in one meta-tree on receipt of SIGUSR2.
3. Trying to analyze the generated CB tree to determine why there are
   so many WORK_* CBs. It looks like there are some internally-generated
   work items chained together to implement readFile. Perhaps it's 
   open-read-close or something. Presumably a library thing.
4. It seems that responding to a client causes Node (V8?) to generate a TIMER_CB. Not sure why. Presumably a library thing.

-----------

13 Jan 2016

1. Checked on the clients associated with the 6 benchmarks. Lowering the
start window to 5 seconds made this more friendly on MUD, though not sure
if this is going to be bad for others. URL is no good, Dr. Lee is working
on it.
2. Refreshed self on status of code base
3. On SIGINT, dump both visualizations (global callback order and tree view) and exit(0)
4. Unified callback: async calls in init stack
   
   Problem: If the application makes an async call in the initial
   stack (as done in the Micro '15 benchmarks code with an IO
   to an output file), then we didn't have a parent. We assumed async
   calls (to the threadpool ) would always have a parent.
   
   Solution: When submitting work to the threadpool, if there is no
   active CBN then we assume we're in the initial stack. Add a special
   function to return a dummy CBN in this case.
5. Now printing the full tree output to a file in /tmp rather than to stdout. This will allow chaining more readily.

-----------

14 Jan 2016

1. On MUD, verified that the number of callback trees is roughly a function of the number of clients. This is good, since it means that I'm not getting totally bogus-looking numbers.
2. Now printing global callback order to a file, along with client ID and start/duration (in seconds since epoch).
3. Discovered that using the fd associated with a handle as a "client id" will not work. In MUD, for example, handles are opened and closed for each http request -- so 5 requests from 8 clients yields 40 open-and-closed handles. The fds will be re-used. Instead we need to really know the client IP and port. Perhaps this can be stored in the "void* uv_handle_t.data" field of a handle when the handle is created by Node.js? Alternatively, more specific subclasses of a handle DO know this info. For example a uv_udp_t has uv_udp_getsockname() for this purpose. Some investigation required.

-----------

15 Jan 2016

1. Wrote a callback statistics calculator, callback_stats. The callback_stats utility calculates interesting statistics on the "all callbacks" file produced by libuv. This can be used to characterize the behavior of Node.js applications, or of a single application across different inputs.

Sample output is below. Note that it is "bogus" in the sense that client calculations are still incorrect (see NEXT STEPS).
However, the total count of callbacks and the per-type counts are accurate.

jamie@suwarna7-Lenovo-K450e:~/bin$ ./callback_stats /tmp/callback_global_order_1452867867_24003.txt
Results from /tmp/callback_global_order_1452867867_24003.txt
------------------------------------------------------------------------------------
Statistic                                                       Value               
------------------------------------------------------------------------------------
Instances of type UV_READ_CB                                    80                  
Client -1: Instances of type UV_ASYNC_CB                        3                   
Instances of type UV__WORK_WORK                                 3                   
Client 12: Instances of type UV_CONNECTION_CB                   40                  
Client 11: Instances of type UV_READ_CB                         80                  
Instances of type UV__IO_CB                                     206                 
Client -1: Instances of type UV_TIMER_CB                        1                   
Client -1: Instances of type UV__WORK_DONE                      3                   
Instances of type UV__WORK_DONE                                 3                   
Client -1: Instances of type UV_FS_CB                           3                   
Instances of type UV__ASYNC_CB                                  3                   
Client -1: Instances of type UV__WORK_WORK                      3                   
Callbacks with 0 children                                       327                 
Instances of type UV_CONNECTION_CB                              40                  
Client 12: number of callbacks                                  40                  
Instances of type UV_ALLOC_CB                                   80                  
Client 11: number of callbacks                                  160                 
Instances of type UV_TIMER_CB                                   1                   
Client -1: Instances of type UV_CLOSE_CB                        40                  
Instances of type UV_FS_CB                                      3                   
Client -1: Instances of type UV__IO_CB                          206                 
Client -1: Instances of type UV__ASYNC_CB                       3                   
Instances of type UV_CLOSE_CB                                   40                  
Client 11: Instances of type UV_ALLOC_CB                        80                  
Client -1: number of callbacks                                  262                 
Callbacks with 1 children                                       55                  
Callbacks with 2 children                                       80                  
Number of callbacks                                             462                 
Instances of type UV_ASYNC_CB                                   3                   
Number of clients                                               3     

-----------

18 Jan 2016

1. Made lists thread-safe internally.
2. Developed a map class for future use. It does not appear to be immediately relevant but "you never know". I thought it would be useful but then I came up with a better approach.
3. Determined that with a tcp handle I can identify the IP and (port? socket?) of the client. This information can be inherited by children. See notes from 1/18.

-----------

19 Jan 2016

1. Debug map class. Debug list class. Add unit tests for each. Commit changes.
2. Make an offline backup of progress so far.
3. Work on client_id. Experiment with lets-chat to see what another set of trees looks like.
  .dot files:
  ls /tmp/*clients.dot
    /tmp/lets-chat_clients.dot  /tmp/lets-chat_manual_clients.dot  /tmp/mud_clients.dot

-----------

20 Jan 2016

1. Implement two client ID maps. The first map is <internal client ID -> hash(sockaddr)>, and the second is <hash(sockaddr) -> "struct sockaddr">. In invoke_callback, if a stream is available then we can (unsafely!) obtain the struct sockaddr associated with it. From this we can determine the client ID. The client ID is based on a hash of the client's IP address and port number.

  On the utility of this approach: 
    mud: The http request embeds the client ID. Each move triggers a new http request (connection) that uses a new port, which causes each user's moves to manifest as a different client. This seems like an inefficient implementation to me. However, the point here is that the application can do "whatever it wants", and this may compromise attempts to identify clients at the framework level.
    lets-chat: There are some unknown client IDs going on here, but the implementation overall has all chat traffic from a user routed through the same connection. I have not tried clicking other buttons. For example, if I start a new chat room, does libuv still know it's me, or does it think I'm a new user?

  A general solution here would be to have one IP per client, and to reduce the client ID to a computation solely based on IP address. This seems like a workable approach, though at the moment it's not good because of the way clients work.
  Another approach would be application-level annotation. This would be undesirable.

2. In progress: once one node in a tree knows its client ID, the entire tree is marked as belonging to that client.

3. Enhance callback_stats to extract and print the "complete schedule" -- the order in which client-driven callbacks (i.e. callbacks that originated from a client).

-----------

21 Jan 2016

1. Think carefully about replayability and tree structures.
2. First draft of correct timer inheritance.

-----------

25 Jan 2016

1. Correct timer inheritance.
2. Color nodes based on client ID.
3. Improved detection of client ID.
4. Add new API uv_init_stack_done so that libuv can distinguish between CBs registered/executed
   in the initial stack vs. those triggered by client input.

-----------

26 Jan 2016

1. Debugging/committing of the 25 Jan. changes.
2. Fix misc compilation warnings.
3. Capture check, prepare, and idle callback registration and inheritance. See src/unix/loop-watcher.c
4. Microsecond granularity for a CBN's "start" field. Otherwise the field was useless.

-----------

27 Jan 2016

I proofread Dr. Lee's TxRace paper for ASPLOS '16 from noon until 11:30 PM.

-----------

28 Jan 2016

1. Mark begin and end of init stack and uv_run. 
Next step: Create a map of callback_address -> source,
where source is one of {NODE_INTERNAL, USER}. Instrumenting the 
callback registration points combined with calls to uv__init_stack_active
and uv__uv_run_active should suffice. 
I am assuming that only one Node implementation thread might interact with uv at a time. We'll see how true that is. If not true, the recourse would be
a per-thread status variable. Easy enough -- note the tid of all threads
that register callbacks.

-----------

29 Jan 2016

No work today.

-----------

1 Feb 2016

1. Complete implementation of origin tracking. This lets me identify CBNs originating from internal Node.js and ibuv code. I can now identify everything but leftover "UV__IO_CBs" that emerge during uv__run_pending. So far I have not seen these register any CBs, so I think this is OK. However, I regret not knowing which CB caused their registration. Perhaps this could be tracked later.
2. Refactor invoke_callback a bit to reduce bloat. Make use of helper functions.
3. Use clock_gettime for a monotonically increasing "start" value. Also use this for the duration calculations. 

-----------

2 Feb 2016

1. Refactor invoke_callback a bit more so that the giant switch is in its own function. Makes invoke_callback more readable.
2. Play on systemG.
3. bitbucket/noderacer: vanilla mode (no Jalangi).
4. Think about uv__run_pending lineage determination.

-----------

3 Feb 2016

1. Play with systemG. Get passwordless ssh, pdsh, bitbucket, etc.
2. Think about uv__run_pending lineage determination.
3. bitbucket/noderacer: vanilla mode (no Jalangi), pick your server IP and port

-----------

4 Feb 2016

1. Implement uv__run_pending lineage determination. Verify that things "look OK" on MUD and lets-chat.
2. Add an assert that threadpool threads only run "expected" CBs (UV__WORK_WORK and UV_WORK_CB).
3. Bugfix: Set init_stack_CBN as the current CBN during the initial stack so that all CBs descend appropriately during the initial stack. 

-----------

NEXT STEPS:
- Where do the unknown CLOSE, SHUTDOWN, and WORK CBs come from in lets-chat?
- Assess flippability calculations I can do "now". Am I missing anything?
- callback_stats: extract the "root schedule". This is order in which *new* external inputs were received from clients, different from the "complete schedule" interplay between the callback trees descending from multiple concurrent external inputs.
- persist the parentage information for callbacks passed to uv__run_pending. These occasionally result in a WRITE, which is bad news bears if we are losing the tree color.
