Notes on ...../node/src
 - node code lives in node/src, JS libraries in node/lib
 - 45 KLOC

node_main.cc::main() -> node.cc::Start -> StartNodeInstance 

StartNodeInstance contains:

Environment* env = CreateEnvironment(isolate, context, instance_data);
...
LoadEnvironment(env);
...

  bool more;
  do { 
    v8::platform::PumpMessageLoop(default_platform, isolate);
    more = uv_run(env->event_loop(), UV_RUN_ONCE);

    if (more == false) {
      v8::platform::PumpMessageLoop(default_platform, isolate);
      EmitBeforeExit(env);

      // Emit `beforeExit` if the loop became alive either after emitting
      // event, or after running some callbacks.
      more = uv_loop_alive(env->event_loop());
      if (uv_run(env->event_loop(), UV_RUN_NOWAIT) != 0)
        more = true;
    }    
  } while (more == true);

------------

So we see that StartNodeInstance calls 'more = uv_run' until more is false.
Does libuv then deal with *everything*? Or are there other routes into node
  that don't go into libuv and are handled directly?

Anyway, it looks like investigating libuv (specifically uv_run) might be more fruitful. 
I think the mental image of Node as "syntax sugar" might be a good one.

-------------------

16 Nov 2015

Question: How are fs.X and fs.XSync implemented?
Answer: X is routed into libuv's thread pool work queue for asynchronous handling. XSync () is routed into libuv where it is called synchronously in uv__fs_work. The result is passed back up.
Relevant code:

We examine stat and statSync, though the implementation looks the same
for all such functions throughout Node.js's JS and C components as well as libuv.

First let's look in lib/fs.js, where we find two versions of every function:

Asynchronous:
  fs.stat = function(path, callback) {
    callback = makeCallback(callback);
    if (!nullCheck(path, callback)) return;
    var req = new FSReqWrap();
    req.oncomplete = callback;
    binding.stat(pathModule._makeLong(path), req);
  };

Synchronous:
  fs.statSync = function(path) {
    nullCheck(path);
    return binding.stat(pathModule._makeLong(path));
  };

The async version creates and passes a "req" object that registers the callback to be called when oncomplete is set. JD: Note that callback might be null, but we still pass *some* callback down to libuv, yes? Might be a null pointer though, and get ignored.
The sync version does not create a "req" object, instead returning the result directly.

Next we examine src/node_file.cc, which appears to correspond to the "binding" referenced in the fs.stat definitions.

Here we have one Stat function:
  static void Stat(const FunctionCallbackInfo<Value>& args) {
    Environment* env = Environment::GetCurrent(args);

    if (args.Length() < 1) 
      return TYPE_ERROR("path required");
    if (!args[0]->IsString())
      return TYPE_ERROR("path must be a string");

    node::Utf8Value path(env->isolate(), args[0]);

    if (args[1]->IsObject()) {
      ASYNC_CALL(stat, args[1], *path)
    } else {
      SYNC_CALL(stat, *path, *path)
      args.GetReturnValue().Set(
          BuildStatsObject(env, static_cast<const uv_stat_t*>(SYNC_REQ.ptr)));
    }
  }

If there is an object in the args (i.e. a req object), we call ASYNC_CALL, otherwise we call SYNC_CALL.

ASYNC_CALL and SYNC_CALL are macros that call ASYNC_DEST_CALL and SYNC_DEST_CALL. These macros are defined as follows, also in src/node_file.cc:
  #define ASYNC_DEST_CALL(func, req, dest, ...)                                 \
    Environment* env = Environment::GetCurrent(args);                           \
    CHECK(req->IsObject());                                                     \
    FSReqWrap* req_wrap = FSReqWrap::New(env, req.As<Object>(), #func, dest);   \
    int err = uv_fs_ ## func(env->event_loop(),                                 \
                             &req_wrap->req_,                                   \
                             __VA_ARGS__,                                       \
                             After);                                            \
    req_wrap->Dispatched();                                                     \
    if (err < 0) {                                                              \
      uv_fs_t* uv_req = &req_wrap->req_;                                        \
      uv_req->result = err;                                                     \
      uv_req->path = nullptr;                                                   \
      After(uv_req);                                                            \
    }                                                                           \
    args.GetReturnValue().Set(req_wrap->persistent());

  #define ASYNC_CALL(func, req, ...)                                            \
    ASYNC_DEST_CALL(func, req, nullptr, __VA_ARGS__)                            \

  #define SYNC_DEST_CALL(func, path, dest, ...)                                 \
    fs_req_wrap req_wrap;                                                       \
    env->PrintSyncTrace();                                                      \
    int err = uv_fs_ ## func(env->event_loop(),                                 \
                           &req_wrap.req,                                       \
                           __VA_ARGS__,                                         \
                           nullptr);                                            \
    if (err < 0) {                                                              \
      return env->ThrowUVException(err, #func, nullptr, path, dest);            \
    }                                                                           \

  #define SYNC_CALL(func, path, ...)                                            \
    SYNC_DEST_CALL(func, path, nullptr, __VA_ARGS__)                            \

So we see ourselves calling uv_fs_<func> (where func is a syscall like stat, close, access, fdatasync, ...) with the env->event_loop () and either the After callback (async) or nullptr (sync).

Looking at libuv now, src/unix/fs.c defines uv_fs_<func> functions for each.
uv_fs_stat is defined as:

  int uv_fs_stat(uv_loop_t* loop, uv_fs_t* req, const char* path, uv_fs_cb cb) {
    INIT(STAT);
    PATH;
    POST;
  }

These macros are:
  #define INIT(subtype)                                                         \
    do {                                                                        \
      req->type = UV_FS;                                                        \
      if (cb != NULL)                                                           \
        uv__req_init(loop, req, UV_FS);                                         \
      req->fs_type = UV_FS_ ## subtype;                                         \
      req->result = 0;                                                          \
      req->ptr = NULL;                                                          \
      req->loop = loop;                                                         \
      req->path = NULL;                                                         \
      req->new_path = NULL;                                                     \
      req->cb = cb;                                                             \
    }                                                                           \
    while (0)
  
  #define PATH                                                                  \
    do {                                                                        \
      assert(path != NULL);                                                     \
      if (cb == NULL) {                                                         \
        req->path = path;                                                       \
      } else {                                                                  \
        req->path = uv__strdup(path);                                           \
        if (req->path == NULL)                                                  \
          return -ENOMEM;                                                       \
      }                                                                         \
    }                                                                           \
    while (0)
...
#define POST                                                                  \
  do {                                                                        \
    if (cb != NULL) {                                                         \
      uv__work_submit(loop, &req->work_req, uv__fs_work, uv__fs_done);        \
      return 0;                                                               \
    }                                                                         \
    else {                                                                    \
      uv__fs_work(&req->work_req);                                            \
      mylog ("POST: result %i\n", req->result);                              \
      return req->result;                                                     \
    }                                                                         \
  }                                                                           \
  while (0)

The key here is POST, which either calls uv__work_submit (when a cb is defined; async request) or uv__fs_work (sync).

uv__work_submit is defined in libuv's src/threadpool.c:
  void uv__work_submit(uv_loop_t* loop,
                       struct uv__work* w,
                       void (*work)(struct uv__work* w),
                       void (*done)(struct uv__work* w, int status)) {
    uv_once(&once, init_once);
    w->loop = loop;
    w->work = work;
    w->done = done;
    post(&w->wq);
  }

  post is also defined in src/threadpool.c, and places the item on the associated wq. This is presumably always the thread pool's work queue:
    static void post(QUEUE* q) {
      uv_mutex_lock(&mutex);
      QUEUE_INSERT_TAIL(&wq, q);
      if (idle_threads > 0)
        uv_cond_signal(&cond);
      uv_mutex_unlock(&mutex);
    }

uv__fs_work, on the other hand, is found in libuv's src/unix/fs.c. It calls the requested syscall directly
and places the result in req->result.

--------------------------------

node/lib/timers:
  setTimeout, setInterval:
    1. Node coalesces timers going off at "about the same time" into a single CB that goes through a linked list of timer CBs.
       I have undone this coalescing.
       bnoordhuis suggests that this is the only case of CB coalescing in Node.
    2. setInterval doesn't make use of the "repeating timer" feature of libuv. If it did so it could not coalesce the timers.
       Consequently each repeating timer is implemented as a sequence of evenly-spaced new timers.

    This means that at the libuv level we cannot see the logical relationship between repeating timers (setInterval)?
    Well, no, the new timer is created while the TIMER_CB of the parent is active. So really repeating timers do
    look like a line of logical children. I confirmed this using timer_repeat.js.

--------------------------------

If you see a signal handler being registered for SIGWINCH, it may come from the first call to console.log. This lets node monitor changes in window size so that the output can be printed more appropriately.

---------------------------------

node::TimerWrap::Now calls uv_update_time.
This causes the value of loop->time to change during a call to uv__run_timers, potentially resulting in the execution of multiple linearly-descended timers (parents, children, grandchildren.) within the same call (provided the timeout for each is small).

---------------------------------

fs.readFile results in 4 UV_WORK events.
fs.writeFile results on 3 UV_WORK events.

Per 'strace -f node fs_read_nest1.js' and 'strace -f node fs_write_nest1.js':
fs.readFile:
  - open
  - stat
  - read (might happen more than once)
  - close
fs.writeFile:
  - open
  - pwrite (might happen more than once)
  - close
This means that to re-order execution we need to do a bit of footwork.
Probably automated using tree tweaks rather than manual file editing.

---------------------------------

Source of the UV_WRITE_CBs that show up intermittently: 
  - When running with mylog enabled, sometimes console.log registers a UV_WRITE_CB
    (e.g. 1x or 2x out of 6 calls in fs_several.c).
  - When running with mylog disabled, I have not seen this happen.
Wandering through the node source code:
  (relevant) callers of uv_write(): StreamWrap::DoWrite
     Callers of this are: StreamBase::Writev
                          StreamBase::WriteBuffer
                          StreamBase::WriteString
  This flavor has stack: StreamBase::JSMethod
                           -> StreamBase::WriteString
                           -> StreamBase::DoWrite
  Examining StreamBase::WriteString in such an instance, it seems that try_write is true.
  In this case we attempt StreamWrap::DoTryWrite (uv_try_write) and only resort to uv_write
    in the event of failure. This failure occurs if we cannot immediately make the write,
    e.g. because there's a pending connect request or because stream->write_queue_size > 0.

Unfortunately, whether or not the stream is ready is beyond my control, so sometimes
  a UV_WRITE_CB might get scheduled.

Proposed solutions:
  1. libuv: Ignore UV_WRITE_CBs that point to StreamWrap::AfterWrite
    > efficient but unsound?
  2. node: Skip the 'try_write' optimization and always file write requests
    > inefficient but sound
    >> I've taken this approach.

--------------------------------------------

Still plagued by UV_WRITE_CB funky parentage issues:
  - changing my code to use fs.writeSync has eliminated that issue
  - the sometime-UV_ASYNC_CB is still around
Question: Why does UV_WRITE_CB sometimes have UV_FS_CB as its parent, and other times not?
Answer: 
  Still unsure. I suspect the cause is buffering at the Node-JS or Node-C++ layer.
  Either way this is unpleasant, since it implies that similar library buffering could be a problem in other settings.
  NB I wouldn't have this issue if scheduling were done at the Node.js level.

A cursory inspection of Node-JS stdio architecture:
  1. In lib/*js, "process.X" means look at node/src/node.js
  2. See   startup.processStdio
    - createWritableStdioStream's for stdout, stderr
    - createReadStream's for stdin
    - sets up SIGWINCH (change in window size) handlers for the streams
  3. createWritableStdioStream DOES guess at the stdio "type" (e.g. terminal vs. a file)
    and obtain a stream to stdio accordingly. This means that it's entirely "reasonable" that
    behavior might change based on "blah" vs. "blah > /tmp/out".

If stdout is redirected to a file, console.log commands are
converetd to fs.writeSync requests.
  src/node.js: 
    case 'FILE':
      var fs = NativeModule.require('fs');
      stream = new fs.SyncWriteStream(fd, { autoClose: false }); 
      stream._type = 'fs';
      break;
  lib/fs.js:
    SyncWriteStream.prototype.write = function(data, arg1, arg2) {
      ...
      fs.writeSync(this.fd, data, 0, data.length);
    }
If stdout is a TTY, console.log commands work as follows:
  src/node.js:
    case 'TTY':
      var tty = NativeModule.require('tty');
      stream = new tty.WriteStream(fd);
      stream._type = 'tty';
      break;
  lib/tty.js:
    function WriteStream(fd) {
      if (!(this instanceof WriteStream)) return new WriteStream(fd);
      net.Socket.call(this, {
        handle: new TTY(fd, false),
        readable: false,
        writable: true
      }); 

      var winSize = []; 
      var err = this._handle.getWindowSize(winSize);
      if (!err) {
        this.columns = winSize[0];
        this.rows = winSize[1];
      }
    }

  const TTY = process.binding('tty_wrap').TTY;
 
  inherits(WriteStream, net.Socket);

  lib/net.js:
    Socket.prototype.write = function(chunk, encoding, cb) {
      if (typeof chunk !== 'string' && !(chunk instanceof Buffer))
        throw new TypeError('invalid data');
      return stream.Duplex.prototype.write.apply(this, arguments);
    };
  lib/stream.js:
    Stream.Duplex = require('_stream_duplex');
  /lib/_stream_duplex.js:
    const Readable = require('_stream_readable');
    const Writable = require('_stream_writable');
  /lib/_stream_writable.js:
    Writable.prototype.write = function(chunk, encoding, cb) {
      ...
      if (state.ended)
        writeAfterEnd(this, cb);
      else if (validChunk(this, state, chunk, cb)) {
        state.pendingcb++;
        ret = writeOrBuffer(this, state, chunk, encoding, cb);
      }
    }
  
    ...

    // if we're already writing something, then just put this
    // in the queue, and wait our turn.  Otherwise, call _write
    // If we return false, then we need a drain event, so set that flag.
    function writeOrBuffer(stream, state, chunk, encoding, cb) {
      ...
      if (state.writing || state.corked) {
        var last = state.lastBufferedRequest;
        state.lastBufferedRequest = new WriteReq(chunk, encoding, cb);
        if (last) {
          last.next = state.lastBufferedRequest;
        } else {
          state.bufferedRequest = state.lastBufferedRequest;
        }
      } else {
        doWrite(stream, state, false, len, chunk, encoding, cb);
      }
      ...

      Eventually clearBuffer will be called, which will handle any
      pending requests.

      // if there's something in the buffer waiting, then process it
      function clearBuffer(stream, state)

      clearBuffer is called by:
        - uncork 
        - onwrite

--------------------------------------------

Looking for process.nextTick and friends? See node/src/node.js

--------------------------------------------

To reduce compiler optimizations, search for -O in node/common.gypi 

--------------------------------------------

If you make any changes to ~/Desktop/node_project/node/
                                                       lib
                                                       src,
you need to recompile for it to take effect. Even if it's .js code

--------------------------------------------

How does process.nextTick work?

Official docs:
https://nodejs.org/api/process.html#process_process_nexttick_callback_arg
  "Once the current event loop turn runs to completion, call the callback function."
Emphasis on "event loop TURN" -- not the entire event loop, just the current turn in it. Basically this means that it will happen after the current function call finishes.
"turn" seems to be synonymous with "tick", used below.

API change notes:
https://github.com/nodejs/node/wiki/API-changes-between-v0.8-and-v0.10
  "process.nextTick happens at the end of the current tick, immediately after the current stack unwinds"

SO clarification:
http://stackoverflow.com/questions/15349733/setimmediate-vs-nexttick/15349865#15349865
  "Use process.nextTick to effectively queue the function at the head of the event queue so that it executes immediately after the current function completes."

Essentially this means that process.nextTick calls are synchronously handled after the end of the current stack.
You can think of them as wrapping the current stack in a larger function call that goes:
  - call function
  - empty the nextTick queue
The function name indicates that the next tick of the loop will be to run the provided function rather than to proceed as normal. 

The synchronicity means that process.nextTick not an interesting area for study.
However, here are some notes on how it works, just in case. 

Test application: next_tick.js
  Two timers are registered to go off after 1 ms.
  Timer 1 calls process.nextTick recursively 3 times.
  Timer 2 just prints to the console.

  When timer 1 goes off (timer_wrap.cc::OnTimeout wrap->MakeCallback),
  1. The timer CB itself runs to completion (async-wrap.cc::MakeCallback cb->Call)
  2. Then env()'s tick_callback_function is called. This is where the registered nextTick functions are handled.

  node.js calls startup.processNextTick at startup. This defines process.nextTick as well as the variables and helper functions needed to support it.
  Notably, it calls into C++ via process._setupNextTick(_tickCallback, _runMicrotasks)
    node.cc::SetupNextTick: - calls env()->set_tick_callback_function to args[1] (JS function _tickCallback (node.js))
                            - adds method 'runMicrotasks' to args[1] (JS object _runMicrotasks, an empty object)

  process.nextTick essentially puts the callback into a queue that gets resolved by _tickCallback every time an object inheriting from async-wrap calls MakeCallback
  (which is pretty much everywhere that callbacks are called, in the various _wrap.cc files).

  Here's the output from the program:
    APP: Program begins
    APP: Loading up some nextTicks
    APP: Done loading up some nextTicks
    APP: nextTick 1 begin
    APP: nextTick 1 done
    APP: nextTick 2 begin
    APP: nextTick 2 done
    APP: nextTick 3 begin
    APP: nextTick 3 done
    APP: Other timeout function
  As you can see, each nextTick function runs to completion before the next one begins, and and timer 1 and all of the nextTick's it registers finish before timer 2 begins.

--------------------------------------------

How does setImmediate work?

It's timers.setImmediate. This places the CB into the immediateQueue, a linked list of CBs that need to be invoked.
The inverse is timers.clearImmediate, which removes the CB from immediateQueue.
Similar to but simpler than regular timers, immediateQueue is a linked list of CBs that need to be invoked, once, "immediately" (but not as immediately as nextTick, which is semi-synchronous).
In setImmediate, process._needImmediateCallback is true and process._immediateCallback is set to function processImmediate

processImmediate is where the queue is handled. 
On return, all callbacks are invoked and immediateQueue is empty.
If a failure is encountered, the remaining CBs are saved and processImmediate is registered using process.nextTick,
giving Node time to crash if needed but still calling them synchronously.

When is it called?

This is a bit trickier than process.nextTick. Here we go!

env.h:  V(immediate_callback_string, "_immediateCallback")
node.cc:
  static void CheckImmediate(uv_check_t* handle) {
    ...
    MakeCallback(env, env->process_object(), env->immediate_callback_string());
    ...
  }

This invokes process._immediateCallback, which is set in timers.js to be timers.js:processImmediate:

  setImmediate{
    ...
    if (!process._needImmediateCallback) {
      process._needImmediateCallback = true;
      process._immediateCallback = processImmediate;
    }
    ...
  }

node.cc:
  The 'check' handle for CheckImmediate is set and disabled here:

  static void NeedImmediateCallbackSetter(
    ...
    if (active) {
      ...
      uv_check_stop(immediate_check_handle);
      ...
    } else {
      ...
      uv_check_start(immediate_check_handle, CheckImmediate);
      ...
    }
    ...
  }

JS accesses to _needImmediateCallback are actually handled by C++ functions:
env.h:  V(need_imm_cb_string, "_needImmediateCallback")
node.cc:
  process->SetAccessor(env->need_imm_cb_string(),
                       NeedImmediateCallbackGetter,
                       NeedImmediateCallbackSetter,
                       env->as_external());

so those changes to process._needImmediateCallback in timers.js execute the associated C++ functions,
which ultimately can lead to calls to process._immediateCallback (aka processImmediate) via a uv_check_t handle called immediate_check_handle that lives in class Environment as immediate_check_handle_.

This means that CBs registered via setImmediate are always carried out in libuv UV_CHECK_CB.
All CBs registered via setImmediate are executed in one shot.

However, unlike timers, Node documents the behavior of setImmediate carefully:

https://nodejs.org/api/timers.html#timers_setimmediate_callback_arg
  Callbacks for immediates are queued in the order in which they were created. The entire callback queue is processed every event loop iteration. If an immediate is queued from inside an executing callback, that immediate won't fire until the next event loop iteration. 
