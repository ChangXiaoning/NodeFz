libuv anatomy (unix only, of course): 

  src/unix/core.c
    --> this is where uv_run is defined.

  The functions that involve executing user code are:
    uv__run_timers
    ran_pending = uv__run_pending
    uv__run_idle
    uv__run_prepare

    uv__io_poll
    uv__run_check
    uv__run_closing_handles

  Description of each core function:
    src/unix/timer.c::uv_run_timers
      --> Go through a heap of timers. Timers are sorted by timeout,
          so we run timers ordered by which one was supposed to go off earliest.
    src/unix/core.c::uv__run_pending
      --> Go through a queue of pending callbacks. Invoke them. Generally not I/O callbacks,
          though possibly.
    src/unix/loop-watcher.c::uv__run_idle (defined by macros, interesting!)
      --> For each of the idle handles, run the callback
          Handles are addressed in sequential order using QUEUE_FOREACH
    src/unix/loop-watcher.c::uv__run_prepare (defined by macros, interesting!)
      --> For each of the prepare handles, run the callback.
          Handles are addressed in sequential order using QUEUE_FOREACH
    uv__io_poll is defined in a few places, depends on the OS.
      src/unix/ aix.c         
                kqueue.c     
                linux-core.c
                sunos.c    
      --> Looking at linux-core.c, housekeeping and then uv__epoll_[p]wait is called
          uv__epoll_[p]wait are defined in src/unix/linux-syscalls.c
          (and basically just call syscall())
          
          Up to 1024 events are consumed at a time. We may wait a bit
          depending on the value of timeout calculated in uv_run.

          We continue to poll if we find 1024 events, this time with no timeout.
          For each event we find, we seem to call a callback on it:
            if (pe->events != 0) {
              w->cb(loop, w, pe->events);
    src/unix/loop-watcher.c::uv__run_check (defined by macros, interesting!)
      --> For each of the check handles, run the callback.
          Handles are addressed in sequential order using QUEUE_FOREACH
    src/unix/core.c::uv__run_closing_handles
      --> For each of the closing_handles in loop->closing_handles, call uv__finish_close,
          which will eventually call handle->close_cb(handle) (running whatever callback
          was defined there).

Miscellaneous notes:
  - Struct declarations seem to be in 'include/uv.h' or 'include/uv-unix.h'
  - For each typedef'd uv_X_t there is a 'struct uv_X_s'.

---------------------

Taken from https://gist.github.com/trevnorris/1f3066ccb0fed9037afa, with some embellishments:
uv__run_pending(): Run all callbacks on the pending_queue. Remove each item from the queue when run.
Note: uv__io_feed() is the only function to insert onto pending_queue.
Note: The following use uv__io_feed():
  - uv_pipe_connect(), but only in the case of an error.
  - uv__write_req_finish(), part of stream.c
      -> e.g. from write (via write2), after writing to the console. Try requiring sys; it will print a warning message and then uv__io_feed.
  - uv_tcp_connect(), but only in the case of an error.
  - uv__udp_send_msg(), for all sent messages.

I think the basic idea is to make sure that this works:

var foo = write('blah', function CB(){ foo.x = 5; });
  -> if we run CB prior to the return of write(), then we'll access
      foo before it is defined. uv__run_pending ensures that we'll
      run after the registering code has completed

---------------------

Ensuring that the current callback node is always safe:
  - threadpool threads enter invoke_callback for one of two callbacks:
      UV_WORK_CB
      UV__WORK_WORK
  - at the moment, I change current_callback_node for all callbacks but UV__WORK_WORK
  - this means that I'm unsafely changing it for UV_WORK_CB, resulting in mangled CB trees

  UV__WORK_WORK: invoked in threadpool.c:worker
  UV_WORK_CB:    invoked in threadpool.c:uv__queue_work with req->work_cb

Threadpool stack: 
  uv_queue_work(work, after): 
    initialize a request
    req->work_cb = work
    req->after = after
    uv__work_submit(req, uv__queue_work, uv__queue_done)

  uv__work_submit:
    uv__work->work = work (i.e. uv__queue_work)
    uv__work->work = done (i.e. uv__queue_done)
    post(w)

  post:
    append the work item to the wq

  worker:
    pop item from wq
    invoke w->work (i.e uv__queue_work) of type UV__WORK_WORK
    put w on the end of the loop's queue for synchronous resolution of done

  uv__queue_work:
    invoke req->work_cb (i.e. the 'work' from uv_queue_work) of type UV_WORK_CB

  uv__work_done:
    Called via indirection along async paths.
      loop.c: err = uv_async_init(loop, &loop->wq_async, uv__work_done);
      uv__work_done invokes w->done for each 'done' work item in loop->wq.

    invoke req->done (which is always(?) uv__queue_done) of type UV__WORK_DONE

  uv__queue_done:
    invoke req->after_work_cb (i.e. the 'after' from uv_queue_work) of type UV_AFTER_WORK_CB

So the asynchronous workflow looks like this:

uv_queue_work (LOOPER THREAD)
   << then eventually >>
  worker::UV__WORK_WORK (THREADPOOL TX, wrapper)
    (has a struct uv__work which is contained in a uv_work_t, see uv__queue_work)
  -> uv__queue_work::UV_WORK_CB (THREADPOOL TX, user's request)
    (has a struct uv__work which is contained in a uv_work_t)
   << then eventually >>
  uv__work_done::UV__WORK_DONE (LOOPER THREAD, wrapper)
    (has a struct uv__work which is contained in a uv_work_t)
  -> if after_work_cb was provided, uv__queue_done::UV_AFTER_WORK_CB (LOOPER THREAD, user's request)
    (has a struct uv__work which is contained in a uv_work_t)

This means that any CB chains resulting from uv_queue_work should look like
  UV__WORK_WORK -> UV_WORK_CB -> UV__WORK_DONE [-> UV_AFTER_WORK_CB]

Happily, all of these structures pass around the same uv__work/uv_work_t,
  so we can embed parentage within.

  Can UV_WORK_CB launch new callbacks? This would be problematic.
  It would be preferable if they can only be launched by UV_AFTER_WORK_CB,
    which is always executed by the LOOPER_THREAD.

  Since UV_WORK_CB is user code, it could very well make calls to libuv.
  I'm not sure if this is relevant in Node.js, but for other users
    of libuv it could happen. The application might use its own locking
    scheme to avoid races on libuv APIs.

  STATUS:
    1. Embed parentage for UV__WORK_WORK -> UV_WORK_CB -> UV__WORK_DONE [-> UV_AFTER_WORK_CB]
    2. Ponder what happens if UV_WORK_CB invokes callbacks (resulting in concurrent invoke_callback calls with arbitrary CBs?)
  

---------

Problem: uv__work_submit is invoked internally. 
Text string: uv__work_submit

  File          Line
  0 threadpool.c  184 void uv__work_submit(uv_loop_t* loop,
  1 threadpool.c  313 uv__work_submit(loop, &req->work_req, uv__queue_work, uv__queue_done);
  2 fs.c          113 uv__work_submit(loop, &req->work_req, uv__fs_work, uv__fs_done);        \
  3 getaddrinfo.c 205 uv__work_submit(loop,
  4 getnameinfo.c 117 uv__work_submit(loop,
  5 uv-common.h   111 void uv__work_submit(uv_loop_t* loop,
  6 fs.c           49 uv__work_submit((loop), &(req)->work_req, uv__fs_work, uv__fs_done);    \
  7 getaddrinfo.c 351 uv__work_submit(loop,
  8 getnameinfo.c 146 uv__work_submit(loop,
This invalidates my "straight line" approach, which only applies to
  the threadpool as used via uv_queue_work.

Threadpool: Possible "work" wrappers (called by UV__WORK_WORK):
  - uv__queue_work
      invokes UV_WORK_CB
  - uv__fs_work
      makes no INVOKE_CALLBACK, just does the request in C
  - uv__getaddrinfo_work 
      makes no INVOKE_CALLBACK, just does the request in C
  - uv__getnameinfo_work
      makes no INVOKE_CALLBACK, just does the request in C

Threadpool: Possible "done" wrappers (called by UV__WORK_DONE):
  - uv__queue_done
      invokes UV_AFTER_WORK_CB
  - uv__fs_done
      invokes UV_FS_CB
  - uv__getaddrinfo_done 
      invokes UV_GETADDRINFO_CB
  - uv__getnameinfo_done
      invokes UV_GETNAMEINFO_CB

Callbacks of these types are only ever called in these contexts. 

Consequently we have the following possible paths:

                       PATH
SOURCE              concurrent       concurrent     looper thread       looper thread
                  UV__WORK_WORK ->              ---> UV__WORK_DONE ->
uv_work_submit                      UV_WORK_CB                       [UV_AFTER_WORK_CB] 
fs.c: a variety                        N/A                              UV_FS_CB
uv_getaddrinfo                         N/A                              UV_GETADDRINFO_CB
uv_getnameinfo                         N/A                              UV_GETNAMEINFO_CB                   

Symbol        Meaning
---------------------
 ->        direct parent-child relationship
 --->      ancestral relationship. Worker threads call uv_async_send once they complete a UV__WORK_WORK CB. This causes the associated loop to eventually call uv__work_done and invoke the corresponding UV__WORK_DONE.

This means:
  - The only user CBs that can be run in parallel are those with UV_WORK_CB. The rest are run in some sequential order by the looper thread.
  - User requests like readFile will be fulfilled in parallel by the fs.c implementation. 

Also note that based on current_callback_node:
  The direct parent of every UV_WORK_CB is a UV__WORK_WORK.
  The direct parent of every UV_AFTER_WORK_CB, UV_FS_CB, UV_GETADDRINFO_CB, and UV_GETNAMEINFO_CB is a UV__WORK_DONE.
Thus the only "missing link" (i.e. can't be handled with current_callback_node) w.r.t. threadpool CBs is the parent of a UV__WORK_DONE CB.
All other relationships can be determined using the per-tid current_callback_node.

Lineage information:
  UV__WORK_WORK: parent is the current_callback_node at uv__work_submit time. Embeds self as the parent during invoke_callback
  UV_WORK_CB: parent is embedded during invoke_callback. Embeds self as the parent during invoke_callback

  UV__WORK_DONE: current_callback_node is always a UV_ASYNC_CB. Logically descends either from a UV_WORK_CB (via uv_work_submit) or a UV__WORK_WORK (via the other 3 paths). Either way the ancestor was embedded. This means that we can climb our own tree until we reach a UV__IO_CB (should always be a root), redirect its parent to the embedded ancestor, and add it to the ancestor's list of children.

  UV_AFTER_WORK_CB: parent is UV__WORK_DONE. Can be retrieved by current_callback_node_get
  UV_FS_CB: parent is UV__WORK_DONE. Can be retrieved by current_callback_node_get
  UV_GETADDRINFO_CB: parent is an active UV__WORK_DONE. Can be retrieved by current_callback_node_get
  UV_GETNAMEINFO_CB: parent is an active UV__WORK_DONE. Can be retrieved by current_callback_node_get

Path from UV__WORK_WORK [UV_WORK_CB] ---> UV__WORK_DONE:
  threadpool::worker calls uv_async_send after the completion of UV__WORK_WORK.
    This results in a call to uv__io_poll, with: UV__IO_CB (uv__async_io) -> UV__ASYNC_CB (uv__async_event) -> UV_ASYNC_CB (uv__work_done) -> UV__WORK_DONE (uv__queue_done, uv__fs_done, uv__ggetaddrinfo_done, uv__getnameinfo_done, leading to UV_AFTER_WORK_CB, UV_FS_CB, UV_GETADDRINFO_CB, UV_GETNAMEINFO_CB respectively)

--------------------------

Questions: 
  What happens in linux-core.c:uv__io_poll? 
  Is there a way to identify the callback that will be invoked, at least to the level of determining the client or the logical parent?

uv__io_poll: eventually calls uv__epoll_wait with loop->backend_fd

backend_fd is set in linux-core.c::uv__platform_loop_init to the result of epoll_create

fds of interest are added to a loop in uv__io_start. They are pushed onto the watcher_queue.
In uv__io_poll, any fds on the watcher_queue are added to the epoll set (using epoll_ctl). These fds are tested each time through the loop until uv__io_close, when uv__platform_invalidate_fd is called to remove it from the epoll set. Due to a possible race, fds are also removed from the epoll set in uv__io_poll if the discovered uv__io_t is NULL.
The callback invoked when they are ready is defined by the callback passed to uv__io_init.

Uses of uv__io_start:

Here's the full list. Details follow.
    File            Function                  Line
  1 async.c         uv__async_start            250 uv__io_start(loop, &wa->io_watcher, UV__POLLIN);
  <omitted, aix.c>
  3 kqueue.c        uv_fs_event_start          407 uv__io_start(handle->loop, &handle->event_watcher, UV__POLLIN);
  4 linux-inotify.c init_inotify               104 uv__io_start(loop, &loop->inotify_read_watcher, UV__POLLIN);
  5 pipe.c          uv_pipe_listen             105 uv__io_start(handle->loop, &handle->io_watcher, UV__POLLIN);
  6 pipe.c          uv_pipe_connect            192 uv__io_start(handle->loop, &handle->io_watcher, UV__POLLIN | UV__POLLOUT);
  7 poll.c          uv_poll_start              115 uv__io_start(handle->loop, &handle->io_watcher, events);
  8 signal.c        uv__signal_loop_once_init  230 uv__io_start(loop, &loop->signal_io_watcher, UV__POLLIN);
  9 stream.c        uv__server_io              536 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLIN);
  a stream.c        uv_accept                  661 uv__io_start(server->loop, &server->io_watcher, UV__POLLIN);
  b stream.c        uv__write                  956 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLOUT);
  c stream.c        uv__read                  1209 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLIN);
  d stream.c        uv_shutdown               1296 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLOUT);
  e stream.c        uv_write2                 1499 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLOUT);
  f stream.c        uv_read_start             1602 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLIN);
  <omitted, sunos.c>
  h tcp.c           uv__tcp_connect            184 uv__io_start(handle->loop, &handle->io_watcher, UV__POLLOUT);
  i tcp.c           uv_tcp_listen              310 uv__io_start(tcp->loop, &tcp->io_watcher, UV__POLLIN);
  j udp.c           uv__udp_send               445 uv__io_start(handle->loop, &handle->io_watcher, UV__POLLOUT);
  k udp.c           uv__udp_recv_start         876 uv__io_start(handle->loop, &handle->io_watcher, UV__POLLIN);

Detail:
1  async.c:uv__async_start (with CB uv__async_io)
    uv_async_init is passed a uv_async_cb.
      it calls uv__async_start (with loop->async_watcher and CB uv__async_event)
        (This adds loop->async_watcher itself to the loop's epoll set, with uv__async_event as the CB)
      then initializes a handle with the uv_async_cb
      then inserts the handle on the tail of the loop's async_handles

      This causes the specified handle
      to be a candidate for having its CB (the uv_async_cb passed to uv_async_init) invoked
      when the fd is signaled. When the async fd is signaled it actually triggers

      The wrapper fd is signaled by a call to uv_async_send, either externally by the user,
      or internally by one of the following:
          0 threadpool.c worker                  105 uv_async_send(&w->loop->wq_async);
          1 threadpool.c uv__work_cancel         222 uv_async_send(&loop->wq_async);
          <omitted, IPHONE> 2 fsevents.c   uv__fsevents_push_event 217 uv_async_send(handle->cf_cb);
          <omitted, apple>  3 stream.c     uv__stream_osx_select   221 uv_async_send(&s->async);
      uv_async_send calls uv__async_send, which writes 1 byte to the async handle->loop->async_watcher if this handle is not yet pending.
      This means that when uv_async_send is called, the next time through uv__io_poll the loop->async_watcher
        will be found ready (with <= N bytes pending where N is the number of attached async handles). 
        Then its associated CB (uv__async_io) will fire, which will exhaust the fd and then invoke its UV__ASYNC_CB (uv__async_event)
        to discover and invoke the UV_ASYNC_CB for each of the send'd fds.

    My understanding is that where possible, libuv will "compress" the number of fds it has to monitor in epoll_wait for performance purposes.
    This cannot be done for individual sockets, but for cases where libuv can be architected to accommodate this goal (e.g. the threadpool done queue, FS event monitoring, and async handles) this is done. 

2. <omitted>

3. uv_fs_event_start: per-handle. http://docs.libuv.org/en/v1.x/fs_event.html implies that each handle will have one callback.

4. linux-inotify.c init_inotify               104 uv__io_start(loop, &loop->inotify_read_watcher, UV__POLLIN);
    This initializes loop->inotify_read_watcher with uv__inotify_read
    uv__inotify_read goes over loop->inotify_fd extracting inotify_event structs and invoking the associated UV_FS_EVENT_CBs

  Looks to me like 3. and 4. are related, but I'm not going to dig into the details now. At any event,
  uv__inotify_read is a wrapper like uv__async_event, compressing multiple watchers into a single fd
    and then invoking the registered CBs when uv__io_poll signals it.

5. pipe.c   uv_pipe_listen
i. tcp.c    uv_tcp_listen
  These are analogous and are called to implement stream.c's uv_listen
  Neither is externally documented.

Considering each...
5. pipe.c uv_pipe_listen
  uv__io_start with a handle->io_watcher set to uv__server_io
  I think this is per-listener and not wrapped, since UV__IO_CB calls uv__server_io directly.
  uv__server_io appears to handle a single callback at a time

i. tcp.c uv_tcp_listen:
  uv__io_start with a tcp->io_watcher set to uv__server_io
  I think this is per-listener and not wrapped, since UV__IO_CB calls uv__server_io directly.
  uv__server_io appears to handle a single callback at a time

Both pipe and tcp call uv__stream_init, which runs uv__io_init with the stream's (pipe's, tcp's) io_watcher and CB uv__stream_io)

uv__stream_io: 
  If there is a connect_req, calls uv__stream_connect (-> UV_CONNECT_CB) and returns
  Else 
    calls uv__read if there's data available
      (which will consume all the input -> sequence of UV_ALLOC_CB and UV_READ_CB calls)
    calls uv__write if ready for output
      (which will emit all the pending output, then call UV_WRITE_CB once)


6. pipe.c   uv_pipe_connect
h. tcp.c    uv_tcp_connect (through uv__tcp_connect)

7 poll.c          uv_poll_start              115 uv__io_start(handle->loop, &handle->io_watcher, events);
8 signal.c        uv__signal_loop_once_init  230 uv__io_start(loop, &loop->signal_io_watcher, UV__POLLIN);
9 stream.c        uv__server_io              536 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLIN);
a stream.c        uv_accept                  661 uv__io_start(server->loop, &server->io_watcher, UV__POLLIN);
b stream.c        uv__write                  956 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLOUT);
c stream.c        uv__read                  1209 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLIN);
d stream.c        uv_shutdown               1296 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLOUT);
e stream.c        uv_write2                 1499 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLOUT);
f stream.c        uv_read_start             1602 uv__io_start(stream->loop, &stream->io_watcher, UV__POLLIN);
<omitted, sunos.c>
j udp.c           uv__udp_send               445 uv__io_start(handle->loop, &handle->io_watcher, UV__POLLOUT);
k udp.c           uv__udp_recv_start         876 uv__io_start(handle->loop, &handle->io_watcher, UV__POLLIN);


-------------

How requests are handled:

Per http://docs.libuv.org/en/v1.x/design.html, handles are long-lived and requests are short-lived. Requests can be used to request some asynchronous work on a handle. The work is specified based on the request API; the request itself has a data pointer for context info and is passed to the CBs.

Requests like uv_fs_write do not use a handle. The handle is, I suppose, implied.

I have *not* yet figured out how a request gets executed. I may need to use gdb, though it's hard to track control flow in an epoll-driven program. In uv__write, for example, the stream maintains a queue of write requests and works its way through them. What causes the stream to acknowledge the pending write request?

------------

Question: Where do "logical" (i.e. user-provided) CBs come from in libuv?
Answer:   Either "as part of a request" or "a response triggered as a result of external input".

Logical trees: This is a list of trees that describe all of the user callbacks that are executed, as well as the connections between them.
Question: How to describe logical trees?
Answer:   Each node can be described with a five-tuple:
  - tree_number (index in the sequence of trees)
  - tree_level (how distant is the root)
  - level_entry (how distant from the first CB in its level)
  - type (allows identification of root nodes)
  - global_id (the order in which it was executed relative to all other callbacks)

Tree indicates the index of the root node of the tree.
The root node of a logical tree is either a request made as part of the initial stack, or responses described for external input.

Example 1:
  In the initial stack, the application registers a repeating timer. 
  The resulting logical tree will look like this:
      INITIAL_STACK -> TIMER_CB -> TIMER_CB -> TIMER_CB -> ...
  Each timer has the same tree, an increasing level, and entry 0.
  Each timer is the logical child of the previous timer.

Example 2:
  In the initial stack, the application starts an HTTP server and registers a callback for connections.
  If three connections come, the resulting logical trees will look like this:
      CONNECTION_CB -> READ_CB -> <tree structure based on the READ_CB>
      CONNECTION_CB -> READ_CB
      CONNECTION_CB -> READ_CB
    
Example 3:
  In the initial stack, the application calls fs.readFile three times.
  The resulting logical tree will look like this:
      INITIAL_STACK  -> WORK_WORK -> UV_FS_CB
                \  \ 
                 \  --> WORK_WORK -> UV_FS_CB
                  ----> WORK_WORK -> UV_FS_CB
      Each WORK_WORK has the same level (1), as does each UV_FS_CB (2)
      The first to be registered is entry 0, then entry 1, then entry 2

Question: How to construct logical trees?
Answer:   For requests, the logical tree can be computed at registration time.
          When the request is submitted, the currently-executing callback is the parent, and the request is appended to the parent's list of children.
          If there is no currently-executing callback, the request must come from internals (Node-C++ or libuv) and is not application code that we 
            need to worry about. Consequently it can be disregarded.

          For responses, the logical tree begins with a CONNECTION_CB or a READ_CB (or analogous CB for non-network events).
          For a root response CB there should be no currently-executing callback.
          Such an event instantiates a new logical tree.
          Once the first event in the tree is executed, the tree can only develop further through requests. This technique is described above.

Question: How to record and replay (or record and play an altered version of events)?
Answer:
  Record: Run the application and feed it whatever external inputs are required.
          When the application ends, emit the logical trees constructed during the execution.
  Replay: Input is a list of logical trees and identical external input.
          Construct the list of logical trees as it emerges.
          Start with global_id = 0
          When we encounter a callback, compare its {tree,level,entry} to those of node with the next-ready global_id.
          If there's a match, invoke the callback and increment the global_id. Iterate over the list of deferred callbacks (sorted by lowest global_id) and re-evaluate each one.
          Otherwise, defer callback execution.
          Optional: After a timeout threshold, increase the current global_id to that of the smallest global_id in the list of deferred callbacks.

  This algorithm requires a few things.
    1. The ability to defer callback execution.
    2. The ability to correctly identify the {tree,level,entry} tuple for pending callbacks. This is relatively straightforward for non-root nodes. For root nodes, things are less clear.

Addressing these requirements:
1. The ability to defer callback execution.

  Attempt 1: I attempted to solve this problem by running libuv as normal, and "skipping over" the invocation of user callbacks.
    User callbacks and their arguments were given to a scheduler thread for execution at the appropriate time.
    This approach failed because libuv and Node-C++/V8 invalidate resources once the libuv code finishes execution.
    For example, libuv would close handles believing that there was nothing more to do for them, while the scheduler thread still held callbacks tied to those handles.
    Consequently, the approach failed.

  Attempt 2: Not yet made. Per the notes above about uv__io_start, I believe that in uv__io_poll I can identify the user-level callback associated with each fd.
    Some fds indicate that one or more sub-events are ready (e.g. async). These "compressing" fds maintain a list of "listeners", so I can still extract the
    listeners that were triggered.
    For non-root logical nodes, I can uniquely determine where in the logical scheme they fall.
    For root logical nodes, things are a bit trickier. Suppose that a server is listening on port 8000 and there are two client connections ready.
    What happens? We have to rely on the underlying APIs to be well behaved. Here's how this "should" work:

      Pending new connections: (a listen()'ing fd that has multiple connection requests):
        man accept: "It extracts the first connection request on the queue of pending connections for the listening socket"
        This means that new inputs will be handled in a FIFO order.
      Pending data on existing connections:
        Each connection has its own socket fd.  
        Based on http://stackoverflow.com/questions/19114001/does-epoll-preserve-the-order-in-which-fds-was-registered and http://lxr.free-electrons.com/source/fs/eventpoll.c,
        it looks like epoll maintains a linked list of ready fds. Consequently the arrival order is preserved for non-wrapped fds. For wrapped fds, the arrival order
        may be unclear, but I expect that we can determine the arrival order ourselves.

        **Is this true if I use level-triggered mode and ignore some of the ready fds? Do they remain at the front of the list or are they popped and pushed?

        Consider async events. These will only be scheduled when uv_async_send is invoked, and so the caller of uv_async_send is the logical parent.
        Now, uv_async_send is thread safe, which means that a "race" from the caller (if application or Node-C++ code) could result in varying logical trees.

        Use of async in the threadpool: 
          The looper thread has an async handle loop->wq_async, whose CB is uv__work_done.
          uv__work_done iterates over loop->wq and invokes the done CB associated with each work item.
          A threadpool worker runs the work item, then adds it to loop->wq and uv_async_send's loop->wq_async.
          So, the threadpool "done" events each uv_async_send the same handle. In a given loop iteration, all but the first one does nothing.
          When the loop's wq_async handle is checked by epoll it is found to be ready, and uv__work_done goes off to invoke the "done" CBs. 

        Conclusion: For the threadpool, we should track the logical parent of the 'WORK' item and the 'DONE' item at uv_queue_work time.
                    For all other async handles, we can track the logical parent of the ASYNC_CB at uv_async_send time.
                    I wonder if application code has any ability to trigger async handles.

        However, I anticipate that wrapped fds (like async events) are associated with non-root nodes, and so the logical ordering is well-known.

      For "compressing" wrappers, if the CBs are all non-root nodes then they can be uniquely identified regardless of order. Are the CBs for "compressing" wrappers always non-root nods?

    Anyway, the idea (which also settles requirement 2) is to simply skip the fd associated with any CB that is not logically next.
    Since libuv appears to use level-triggered EPOLL, there would be no need to re-arm the fds that are skipped. They will just go off until I'm ready to handle the CB.

    For "fd skipping", we assume that each fd (or sub-fd in the case of wrapping fds) is associated with exactly one logical callback in its execution stack.
    libuv does not support the registration of a sequence of callbacks, so this assumption is sound. Users who want a sequence have to embed it
      within the callbacks they register. The only exception is READ_CB, which calls an ALLOC_CB beforehand. However, ALLOC seems quite ignorable.
    
    The "physical trees" I've been collecting so far have included all of the internal CBs: UV__ASYNC_CB, UV__IO_CB, etc.
    In terms of analyzing user code, user CBs are all that matter. The "physical trees" are unnecessary, since each one will now contain
    exactly one user CB. The user CB will imply its physical parents. Call it "tree collapsing".

    User CBs are only registered by Node-C++, libuv, or other user CBs. This means that at registration time we can identify the logical
      parent and be good to go.

    Visualization:
      Timeline -- this will show the chronological order in which user CBs were invoked, and takes the place of the physical trees.
      The relationship graph I've been generating will show only the logical trees.
      Any non-user CB is just a distraction.

    Question: How do I differentiate between multiple callbacks that execute the same code?
      Using timers.js, I see that Node re-uses the compiled version of JS functions. The same callback is registered 20 times in uv_timer_start. 
    Answer: At registration time we know the parent. We establish a logical node describing the child, and save it somewhere that the child can
            reach it. For one-CB handles we can save it in the parent. For multi-CB things like timers or workers, we can save it in the timer/worker structure.
            This is similar to how we embed parentage at the moment.

      
--------------------

Places where callbacks are registered:

cscope with an extra "Registrant info" column:
Functions calling this function: uv__register_callback

  File           Function               Line                                              Registrant info
0 fs-poll.c      uv_fs_poll_start         69 uv__register_callback(cb,                    Handle of type uv_fs_poll_t is specified. UV_FS_POLL_CB gets the handle.
                                             UV_FS_POLL_CB);
1 threadpool.c   uv_queue_work           305 uv__register_callback(work_cb,               uv_work_t (req) is provided
                                             UV_WORK_CB);
2 threadpool.c   uv_queue_work           306 uv__register_callback(after_work_cb          Ditto
                                             , UV_AFTER_WORK_CB);
3 core.c         uv_close                102 uv__register_callback((void *)               uv_handle_t handle. close_cb gets the handle.
                                             close_cb, UV_CLOSE_CB);
4 fs.c           INIT                     63 uv__register_callback(cb,                    This is a macro used by the FS wrappers. There's a uv_fs_t req. uv_fs_cb gets the req.
                                             UV_FS_CB); \
5 getaddrinfo.c  uv_getaddrinfo          176 uv__register_callback(cb,                    There's a uv_getaddrinfo_t req. uv_getaddrinfo_cb gets the req.
                                             UV_GETADDRINFO_CB);
6 kqueue.c       uv_fs_event_start       377 uv__register_callback(cb,                    uv_fs_event_t handle and uv_fs_event_cb gets it.
                                             UV_FS_EVENT_CB);
7 loop-watcher.c UV_LOOP_WATCHER_DEFINE   35 uv__register_callback(cb,                    check, idle, prepare: Handle of each type. cb gets the handle.
                                             UV_##type##_CB); \
8 pipe.c         uv_pipe_connect         156 uv__register_callback(cb,                    uv_connect_req and CB gets it.
                                             UV_CONNECT_CB);
9 poll.c         uv_poll_start           106 uv__register_callback(poll_cb,               uv_poll_t handle and CB gets it.
                                             UV_POLL_CB);
a process.c      uv_spawn                518 uv__register_callback(process->exit          uv_process_t handle and CB gets it.
                                             _cb, UV_EXIT_CB);
b signal.c       uv_signal_start         306 uv__register_callback(signal_cb,             uv_signal_t handle and CB gets it.
                                             UV_SIGNAL_CB);
c stream.c       uv_listen               672 uv__register_callback((void *) cb,           uv_stream_t handle and CB gets it.
                                             UV_CONNECTION_CB);
d stream.c       uv_shutdown            1286 uv__register_callback((void *) cb,           There's a uv_shutdown_t req and CB gets it.
                                             UV_SHUTDOWN_CB);
e stream.c       uv_write2              1448 uv__register_callback((void *) cb,           There's a uv_write_t req and CB gets it.
                                             UV_WRITE_CB);
f stream.c       uv_read_start          1583 uv__register_callback((void *)               uv_stream_t handle and CB gets it. ALLOC can probably be ignored.
                                             alloc_cb, UV_ALLOC_CB);
g stream.c       uv_read_start          1584 uv__register_callback((void *)               uv_stream_t handle and CB gets it. READ should be handled.
                                             read_cb, UV_READ_CB);
h timer.c        uv_timer_start           72 uv__register_callback(cb,                    There's a uv_timer_t handle and CB gets it.
                                             UV_TIMER_CB);
i uv-common.c    uv_tcp_connect          292 uv__register_callback((void *) cb,           There's a uv_connect_t req and CB gets it.
                                             UV_CONNECT_CB);
j uv-common.c    uv_udp_send             318 uv__register_callback((void *)               There's a uv_udp_send_t req and CB gets it.
                                             send_cb, UV_UDP_SEND_CB);
k uv-common.c    uv_udp_recv_start       353 uv__register_callback((void *)               There's a uv_udp_t handle and CB gets it. ALLOC can probably be ignored.
                                             alloc_cb, UV_ALLOC_CB);
l uv-common.c    uv_udp_recv_start       354 uv__register_callback((void *)               There's a uv_udp_t handle and CB gets it. READ should be handled.
                                             recv_cb, UV_UDP_RECV_CB);
m uv-common.c    uv_walk                 374 uv__register_callback((void *)               This prompts a synchronous series of UV_WALK_CB calls. There's not an obvious way to capture this one. Node seems to use it only once, to close the handles in a loop prior to exit, so I'm not particularly worried about it.
                                             walk_cb, UV_WALK_CB);


What do we learn from this?
  At any given time, any given handle type can have at most one CB that will be invoked with the handle itself as an arg.
  At any given time, any given handle type may have several pending req's, each of which will be called with the req as an arg.
  At any given time, any given req type can have at most one CB associated with its completion.
This suggests that embedding the logical_node in the handle or the req should work.

Use callback_type_to_context (HANDLE or REQ) and callback_type_to_behavior (ACTION or RESPONSE) to differentiate between the types of callbacks.

The notes in callback_type_to_behavior indicate that each request and each handle has only one callback per callback type. However, a handle may have multiple types of callbacks pending. Consequently storing a single lcbn in the handle would not work. Instead we can store a map of cb_type->lcbn in the handle; there is at most one CB of each type that the handle supports. We can do the same for the request (though only one type is expected) so that the code to retrieve the lcbn can be the same.


-------------------

Steps to add new c files to libuv:
  
- create the header (if any)
- create the c file
- include the header where needed
- add the c file to uv/uv.gyp appropriately. Don't forget the comma at the end of the line!
- go back to the root and run './configure --debug' (not sure if this is necessary?)

node/node.gyp references uv's uv.gyp file

Note that if you don't run './configure --debug', you won't have debug info for node/src.

-------------------

Node does not make use of the "repeating timers" feature of libuv, so there's no need to make lcbn inheritance work at the moment.
I added an assert that no repeating timers are used in unix/timer.c

-------------------

linux-core.c:uv__io_poll
  Callbacks are invoked as

#if UNIFIED_CALLBACK
  INVOKE_CALLBACK_3(UV__IO_CB, w->cb, loop, w, pe->events);
#else
  w->cb(loop, w, pe->events);
#endif

w is a uv__io_t:
  struct uv__io_s {
  uv__io_cb cb; 
  void* pending_queue[2];
  void* watcher_queue[2];
  unsigned int pevents; /* Pending event mask i.e. mask at next tick. */
  unsigned int events;  /* Current event mask. */
  int fd; 
  UV_IO_PRIVATE_PLATFORM_FIELDS
  struct callback_node *logical_parent; /* The current CBN at the time of uv__io_feed. That parent always completes before CB is executed. */
};

The cb is of type uv__io_cb. The functions that can be registered as w->cb are:
  Text string: loop, uv__io_t

      File       Line
  0 aix.c       705 static void uv__ahafs_event(uv_loop_t* loop, uv__io_t* event_watch, unsigned int
                    fflags) {
  1 async.c     122 static void uv__async_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  2 core.c      848 void uv__io_start(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  3 core.c      881 void uv__io_stop(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  4 core.c      913 void uv__io_close(uv_loop_t* loop, uv__io_t* w) {
  5 core.c      922 void uv__io_feed(uv_loop_t* loop, uv__io_t* w) {
  6 internal.h  178 void uv__io_start(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  7 internal.h  179 void uv__io_stop(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  8 internal.h  180 void uv__io_close(uv_loop_t* loop, uv__io_t* w);
  9 internal.h  181 void uv__io_feed(uv_loop_t* loop, uv__io_t* w);
  a internal.h  204 void uv__server_io(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  b kqueue.c     37 static void uv__fs_event(uv_loop_t* loop, uv__io_t* w, unsigned int fflags);
  c kqueue.c    307 static void uv__fs_event(uv_loop_t* loop, uv__io_t* w, unsigned int fflags) {
  d poll.c       30 static void uv__poll_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  e signal.c     41 static void uv__signal_event(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  f stream.c     68 static void uv__stream_io(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  g stream.c    527 void uv__server_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  h stream.c   1304 static void uv__stream_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  i udp.c        41 static void uv__udp_io(uv_loop_t* loop, uv__io_t* w, unsigned int revents);
  j udp.c       134 static void uv__udp_io(uv_loop_t* loop, uv__io_t* w, unsigned int revents) {

Assessing each:
  0 aix.c       705 static void uv__ahafs_event(uv_loop_t* loop, uv__io_t* event_watch, unsigned int fflags) {
    Omitted, aix

  1 async.c     122 static void uv__async_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
    Wrapper. Empties the fd (it's a pipe, I think?) and then calls uv__async_event for those handles that were send'd.
             Wraps any async handles registered by the user as well as pending threadpool WORK_DONE events.

  f signal.c    350 static void uv__signal_event(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
    Wrapper. Empties the fd (it's a pipe) and then calls uv__signal_cb for those signals that were received.
    (Signals are external input, so on identical inputs we should get identical ordering.)

  2 core.c      848 void uv__io_start(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  3 core.c      881 void uv__io_stop(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  5 core.c      922 void uv__io_feed(uv_loop_t* loop, uv__io_t* w) { 
    I do not believe these are functions that could be called from uv__io_poll.

  4 core.c      913 void uv__io_close(uv_loop_t* loop, uv__io_t* w) { 
    Let's not worry about this for now.

  6 internal.h  178 void uv__io_start(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  7 internal.h  179 void uv__io_stop(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  8 internal.h  180 void uv__io_close(uv_loop_t* loop, uv__io_t* w);
  9 internal.h  181 void uv__io_feed(uv_loop_t* loop, uv__io_t* w);
  a internal.h  204 void uv__server_io(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  b kqueue.c     37 static void uv__fs_event(uv_loop_t* loop, uv__io_t* w, unsigned int fflags);
  e signal.c     41 static void uv__signal_event(uv_loop_t* loop, uv__io_t* w, unsigned int events);
  j udp.c        41 static void uv__udp_io(uv_loop_t* loop, uv__io_t* w, unsigned int revents);
    Declarations, not definitions

  c kqueue.c    307 static void uv__fs_event(uv_loop_t* loop, uv__io_t* w, unsigned int fflags) {
  d poll.c       30 static void uv__poll_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
    Associated with a single handle. Easily extracted.

  g stream.c     68 static void uv__stream_io(uv_loop_t* loop, uv__io_t* w, unsigned int events);
    While there are connections pending, calls UV_CONNECTION_CB.
    (This is external input, so on identical inputs we should get identical ordering of pending connections.)

  h stream.c    527 void uv__server_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
  i stream.c   1304 static void uv__stream_io(uv_loop_t* loop, uv__io_t* w, unsigned int events) {
    This is associated with a single stream. 
    Triggers either "connect" or 
      'read? read!' and 'write? write!'.
        More specifically, the steps seem to be: 
          "if read is ready, make all the reads" (uv__read) 
          "if write is ready, make one(? Looks like a TODO "TODO: start trying to write the next request") write and then call the cb (UV_WRITE_CB) for all reqs on stream->write_completed_queue". (via uv__write, uv__write_callbacks)
          "if no write requests left and the stream has been closed, close the connection" (uv__drain)

  k udp.c       134 static void uv__udp_io(uv_loop_t* loop, uv__io_t* w, unsigned int revents) {
    Similar to uv__stream_io.
    This is associated with a single stream. 
    Triggers 'read? read!' and 'write? write!'.
        More specifically, the steps seem to be: 
          "if read is ready, make all the reads, calling a sequence of UV_ALLOC_CB and UV_READ_CB" (uv__udp_recvmsg) and 
          "if write is ready, make all the writes and then call all of the UV_WRITE_CBs" (uv__udp_sendmsg, uv__udp_run_completed)

Conclusion from this exercise:
  Based on the value of the cb, I should be able to write APIs to determine the possible LCBNs available from each uv__io_t *w.
  This would let me ask the scheduler whether or not I should run w.
  An alternative design would be to put this code in the possible UV__IO_CB functions. The first thing that they would do is
  check whether their set of potential LCBNs includes the "next" LCBN. If so, they would execute only that one, and ultimately
  return a value indicating whether or not they should be re-armed.
  
  It would be preferable to have not a set of LCBNs but rather a single LCBN. How difficult would it be to break up the wrappers?
  Would this work, or would I still be caught by things like uv__stream_io and uv__udp_io that contain a list of requests?

  Let's develop the scheduling API using something simple to start with: timers

Proposed scheduling API:
  Input: LCBN
  Output: Whether or not it is the next-to-be-invoked
         This is certainly the most basic API.

  Input: context, cb_type
  Output: Whether or not the suggested CB is the next-to-be-invoked

  Input: context, type (handle or req)
  Output: Whether or not the suggested handle/req contains the next-to-be-invoked LCBN

The scheduler gets as input a list of LCBNs of the form {tree,level,entry,type,id}.
It sets global_id to 0.
Every time it invokes a callback it increments global_id.
To determine the next callback to invoke, it looks at the next LCBN in its schedule.
It compares this "next" LCBN against pending LCBNs until it finds a match. The 'type' is needed to identify the next root node correctly. It could be a UV_CONNECTION_CB, UV_FS_EVENT_CB, etc. (any of the CALLBACK_BEHAVIOR_RESPONSE cb_types), but with fixed inputs the first matching CB in the list of pending CBs from epol should be a match due to the FIFO nature of epoll.

------------------

Need to track the work/done CBs for all threadpool use, not just via uv_queue_work.
As noted earlier, the following places make use of uv__work_submit (other than uv_queue_work itself):

1 fs.c          POST            113 uv__work_submit(loop, &req->work_req, uv__fs_work, uv__fs_done); \
2 getaddrinfo.c uv_getaddrinfo  205 uv__work_submit(loop,
3 getnameinfo.c uv_getnameinfo  117 uv__work_submit(loop,

Want to capture the invocation of the 'WORK' CB. How to deal with these?
Options:
  1. Conditional tracking of the work CB in uv__work_submit. We can identify people not coming from uv_queue_work and track only their UV__WORK_WORK. This seems like a good option: preserve the most info.
  2. Always-on tracking of CBs in uv__work_submit rather than/in addition to the callers.
    > "Rather than" -- lose info about what type of work request it was: FS vs. user vs. ...
    > "In addition to" -- every uv_queue_work work item produces two CBs. Clutter.
  3. Wrap the "work" CB and route them through uv_queue_work.
    > Looks mildly tricky because the structs are different. Cleanest option. Add CB types for the 'work' components.
    > Initial implementation for GETADDRINFO works. Huzzah! Note that *every* async request will now produce UV_WORK_CB and UV_DONE_CB requests. This seems OK. Can patch up later with 'other dependencies'. Need to do this anyway. 
    > I completed changes for GETNAMEINFO and FS too.

------------------

uv__run_closing_handles:
  - src/unix/core.c

  uv_handle_t* p;
  uv_handle_t* q;

  p = loop->closing_handles;
  loop->closing_handles = NULL;

  while (p) {
    q = p->next_closing;
    uv__finish_close(p);
    p = q;
  }

void uv__make_close_pending(uv_handle_t* handle) {
  assert(handle->flags & UV_CLOSING);
  assert(!(handle->flags & UV_CLOSED));
  handle->next_closing = handle->loop->closing_handles;
  handle->loop->closing_handles = handle;
}

UV_LOOP_PRIVATE_FIELDS:
  uv_handle_t* closing_handles;

#define UV_HANDLE_PRIVATE_FIELDS
  uv_handle_t* next_closing;

So what we've got here is a singly-linked list. We can run uv__finish_close on a subset of the handles if desired.
uv__finish_close:
    switch (handle->type) {
    case UV_PREPARE:
    case UV_CHECK:
    case UV_IDLE:
    case UV_ASYNC:
    case UV_TIMER:
    case UV_PROCESS:
    case UV_FS_EVENT:
    case UV_FS_POLL:
    case UV_POLL:
    case UV_SIGNAL:
      break;

    case UV_NAMED_PIPE:
    case UV_TCP:
    case UV_TTY:
      uv__stream_destroy((uv_stream_t*)handle);
      break;

    case UV_UDP:
      uv__udp_finish_close((uv_udp_t*)handle);
      break;
    }
    if (handle->close_cb)
      INVOKE_CALLBACK_1 (UV_CLOSE_CB, handle->close_cb, handle);

  So, for UV_PREPARE...UV_SIGNAL it's easy to see what CBs might be invoked.
  For the rest it's a bit more complex.

  UV_NAMED_PIPE, UV_TCP, UV_TTY:
    uv__stream_destroy:

      if(stream->connect_req)
        INVOKE_CALLBACK on the UV_CONNECT_CB with -ECANCELED
      uv__stream_flush_write_queue -> this moves all pending write requests to the done queue with -ECANCELED
      uv__write_callbacks(stream); -> this invokes the UV_WRITE_CB of all of the pending write requests that had one
      if (stream->shutdown_req)
        INVOKE_CALLBACK on the UV_SHUTDOWN_CB with -ECANCELED

  UV_UDP:
    uv__udp_finish_close:
      empty the write_queue with status -ECANCELED, then call
      uv__udp_run_completed -> this invokes all of the handle->write_completed_queue requests on UV_UDP_SEND_CB with the recorded status (-ECANCELED for the ones we just emptied out of the write queue)

----------

uv__run_pending:
  Exhausts loop->pending_queue, which contains uv__io_t's.
  We invoke w->cb(loop, w UV__POLLOUT)  each uv__io _t.

  w's get into loop->pending_queue via uv__io_feed.
  Callers:
    - uv__write_req_finish(), part of stream.c
        -> e.g. from write (via write2), after writing to the console. Try requiring sys; it will print a warning message and then uv__io_feed.
      ?
    - uv_pipe_connect(), but only in the case of an error.
      ?
    - uv_tcp_connect(), but only in the case of an error.
      ?
    - uv__udp_send_msg(), for all sent messages.
      DONE

pipe, tcp:
  uv_X_init calls uv__stream_init
    uv__stream_init calls uv__io_init with the stream's io_watcher and uv__stream_io
    When pipe
udp:
  uv_udp_init[_ex] calls uv__io_init with the udp handle's io_watcher and uv__udp_io

Conclusion: in uv__run_pending, the value of w->cb should be uv__stream_io or uv__udp_io. The handle should be extractable based on the value.

Portions of uv__stream_io relevant to UV__POLLOUT:
  if (stream->connect_req)
    uv__stream_connect
    return
  if (uv__stream_fd(stream) == -1)
    return; 
  uv__write 
    -- first request off of stream->write_queue
  uv__write_callbacks 
    -- all CBs from stream->write_completed_queue
  if there was only one CB in stream->write_queue, uv__drain
    -- if(stream->shutdown_req) (this is a valid thing to base decision on due to assert in uv__drain), honor it
      

----------

async handles:
  - no internal-API-only users (unlike the way FS and DNS used the threadpool under the hood)
  - the only internal use is for recognizing pending 'done' items from the threadpool, and it does so using the same APIs that external users do

uv_async_init(handle) 
  - calls uv_async_start with &loop->async_watcher, with CB uv__async_event. uv_async_start calls uv__io_init with loop->async_watcher and CB uv__async_io
  - places the handle at the end of the queue loop->async_handles
uv_async_send(handle)
  - if handle is pending, do nothing (coalesces send's)
  - else, set handle to pending, then uv__async_send to loop->async_watcher
    (this writes a byte to the shared pipe)

  Users can call this directly on their async handles to trigger the async CB associated with it.
  The threadpool calls it after completing a work item and adding it to the loop's 'done' list.

uv__io_poll: The only async fd added to the loop->backend_fd is the one associated with loop->async_watcher.
             Thus, uv_async_send'ing on any registered async handle 'sets' only one fd in loop->backend_fd.
             This wrapping requires identifying which async handle(s) actually triggered the umbrella fd...
uv__async_io: Empties the pipe, then calls uv__async_event. 

uv__async_event:
  This goes through all of the registered async handles looking for pending ones, and calls the associated CB. 
  For the threadpool, this CB is uv__work_done, which goes through the loop's "done queue" and invokes all of the CBs.
  If any async handles are deferred by the scheduler, we uv_async_send again to ensure we have the opportunity to schedule them later.

uv__work_done: 
  - Executes any of the pending 'done' CBs according to the schedule.
  - Spins waiting for 'done' CBs if the next scheduled CB is an AFTER_WORK CB and it is not yet availble
  - If any 'done' items are deferred by the scheduler, we uv_async_send again to ensure we have the opportunity to schedule them later.

Scheduling: 
  - Because the threadpool uses the same APIs that external users do, the ASYNC_CB that goes off when there is a 'done' item in the threadpool
    is included counted in the user-CB schedule.
  - This means that if we defer events threadpool events, we need to add new ASYNC_CBs at appropriate spots to obtain a valid schedule.

  Example: Suppose we have the schedule: WORK1, WORK2, ASYNC, DONE1, DONE2, and we want to invoke DONE1 before WORK2.
    The new schedule is:
      WORK1, ASYNC, DONE1, WORK2, ASYNC, DONE2
    Note the addition of a second ASYNC CB to ensure that we can get to DONE2.
    This will require "shifting" exec IDs in the second schedule.

-----------------

uv memory allocation:
  uv__malloc, uv__free, uv__calloc, uv__realloc:
    wrappers for (static) 'uv__allocater.local_X'
  Wrappers can call uv_replace_allocator and replace the allocator functions
  with their own equivalent ones if they want to use another memory management scheme.

  The defaults are libc malloc/free/calloc/realloc.
  Node.js does not call uv_replace_allocator, so libuv uses the vanilla options.

-----------------

uv__work_cancel:
  work items that can be cancelled (those not yet grabbed by a threadpool thread and not yet executed) have their work set to uv__cancelled and are added to loop->wq to get picked up by uv__work_done. In uv__work_done, if w->work == uv__cancelled then w->done is invoked with UV_ECANCELED

-----------------

ASYNC_CBs are invoked when a signal'd async object is handled.
These are used internally, too:
  - threadpool (ASYNC_CB -> uv__work_done -> UV_AFTER_WORK_CBs) 
  - fsevents   (ASYNC_CB -> uv__fsevents_cb -> UV_FS_EVENT_CBs)
UV_FS_EVENT_CBs are also invoked in linux-inotify.c (in uv__inotify_read, which is registered as the loop's inotify_read_watcher)

When considering schedule validity, here are some rules related to TP 'done' events:
  UV_AFTER_WORK_CBs (executed when the threadpool finishes a job) can legally be preceded by one of the following types:
    - A UV_ASYNC_CB in the TP's chain of UV_ASYNC_CBs
    - UV_AFTER_WORK_CB      (predecessor had no "nested" children; an external job submitted via uv_queue_work)
    - UV_FS_CB              (predecessor was uv__fs_done)
    - UV_GETADDRINFO_CB     (predecessor was uv__getaddrinfo_done)
    - UV_GETNAMEINFO_CB     (predecessor was uv__getnameinfo_done)

  UV_{FS,GETADDRINFO,GETNAMEINFO}_CBs can legally be preceded only by a UV_AFTER_WORK_CB

  UV_AFTER_WORK_CBs (executed when the threadpool finishes a job) can legally be followed by one of the following types:
    - MARKER_IO_POLL_END    (no more 'done' events)
    - UV_AFTER_WORK_CB      (no "nested" children; an external job submitted via uv_queue_work)
    - UV_FS_CB              (uv__fs_done)
    - UV_GETADDRINFO_CB     (uv__getaddrinfo_done)
    - UV_GETNAMEINFO_CB     (uv__getnameinfo_done)
    - any other IO poll CBs (yuck, so looking forward is harder than looking backward)

-----------------

The order in which children of the INITIAL_STACK are registered:
  2 internal UV_ASYNC_CBs (the 'TP done' chain first)
  Then its misc. 'application children'
  Second to last is the UV_RUN_BEGIN marker.
  Last (and much later in the exec schedule) is the EXIT marker.
In rescheduling, we need to get the relative child number correct for all non-marker children.
For markers we don't check the registration ID, just the type, because they are unique.
