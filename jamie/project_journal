Jamie Davis <davisjam@vt.edu>
22 October 2015

I went into src/threadpool.c and added printfs to 
 work
  > this actually performs a work item
    calls w->work
 uv__work_done
  > invokes the 'done' callback on a completed work item
    calls w->done

 include/uv-threadpool.h defines the uv__work item, with
 the work and done fields defined as:

   void (*work)(struct uv__work *w);
   void (*done)(struct uv__work *w, int status);

FORWARD ORDER
1
stat hello 0x1897380
stat world 0x1897aa0
rename 0x1896a40

2
stat hello 0x19cb380
rename 0x19caa40
stat world 0x19cbaa0

3
stat hello 0x1539380
stat world 0x1539aa0
rename 0x1538a40

4
stat hello
rename
stat world

5
stat hello
stat world
rename

6
stat hello
stat world
rename

7
stat hello
stat world
rename

8
stat hello
stat world
rename

REVERSED ORDER
1
rename 0x13c8848
stat world 0x1663aa0
stat hello 0x1663380

2
rename 0x2873a40
stat world 0x2874aa0
stat hello 0x2874380

3
rename
stat world
stat hello

4
rename
stat world
stat hello

5
stat world
rename
stat hello

------------------

Conclusion:
  0. file_system.js is a useful tool to explore node+libuv interactions.

  1. The order in which callbacks enter the queue in uv__work_done
    depends on the order in which the underlying FS operations complete.

  2. Once they reach the queue, we can re-order them, as demonstrated.
    However it's the thread pool ordering that determines the order 
    in which they reach the queue.
    I haven't yet identified a way to determine which callback is which
    across runs (i.e. which one is 'stat' vs 'rename' in run X?).
    This would allow me to re-order function and callback invocations.
    
  3. To do that, I think I need to know how 'work' and 'done' get set
    for each `struct uv__work'.

  4. Just because a 'stat' command succeeded does not mean that the 
     stat'd file still exists at the time of the callback.

  5. Can re-order things all we want (both threadpool execution order 
      and global callback order) in src/threadpool.c, PROVIDED
      we can identify what we're working with.

  6. The libuv forum suggests that the threadpool is only used to
      emulate asynchronous-less synchronous operations like AIO.
      Where are asynchronous-native operations like sockets handled?

Open questions:
  1. Where are asynchronous-native operations like sockets handled?
  2. How does node.js submit operation+callback requests to libuv?
      >> $NODE/src/*wrap.cc ?
  3. Can we add user-defined data to libuv structs for label propagation?

---------------------------------

Jamie Davis <davisjam@vt.edu>
23 October 2015

Yesterday I discussed the open questions above with Dr. Lee.
#1: He suggested that finding this out would be valuable.
#2, #3: This may be irrelevant. If a fork (and-wait?) approach is used,
  the parent can launch a child to explore the path not taken.
  In such a setting, there's no need to know WHAT the work payload is,
  because the memory address itself can be used as the ID.
  I was thinking that we had to propagate labels in order to identify items
  between runs, but if I stick to a PARTICULAR run then I can use fork to
  reorder arbitrarily. 
  
  See test_code/fork_test.c and test_code/fork_test.txt for some sample code 
    along these lines.

  New open questions:
  1. Where are asynchronous-native operations like sockets handled?
     Is every callback executed in uv__work_done?
      > Explore http server, since socket code is not done by threadpool.
        I believe the answer is yes.
  2. Play with fork!

-------------------------

24 October 2015

Addressing the open questions from 23 October:

1. Regarding asynchronous-native operations like sockets:
   The callback is executed in uv__io_poll.
   Testing with 'node simple_http':
     uv__io_poll: calling w 0x2bd84a8's cb 0xdbff40 with {events 1, data 13}
     APP: Server handled a client!
     uv__io_poll: finished calling w 0x2bd84a8's cb 0xdbff40 with {events 1, data 18446744073709551615}

   The callback function is:
     gdb `which node`
     info symbol 0xdbff40
     uv.stream_io in section .text

     src/unix/stream.c:

   Uncomfortably, multiple callbacks can be pending in each phase of the loop, and furthermore
     in uv__io_poll we may call a single callback that routes us into threadpool: uv__work_done
     which calls multiple callbacks.
     It's not clear how to change the order of these callbacks at the level of uv__io_poll.

   Suppose that for starters we only re-order callbacks in threadpool::worker and threadpool::uv__work_done.
   This should be enough to see some interesting behavior.

2. Playing with fork: 
   I modified uv__work_done as follows:
    - wait until 4 callbacks show up (not sure what they all are)
    - shmem
    - fork()
      > child goes first and runs callbacks in forward order
      > parent sees child is done, goes second, and runs callbacks in reverse order
    Result: success, callbacks are executed in forward and reverse order as expected.

   This raises new questions.

    1. Unlike race conditions in memory, in node the race conditions can and do modify external
       resources like the file system or databases. 

       Different copies must not be allowed to see each other's actions. This suggests the need
       to ``reset'' things to the previous state. In the filesystem, taking a snapshot/rsync-backup
       should suffice (provided it preserves things like atime "just in case"). The database presumably
       also live in the filesystem. Network traffic can't be rolled back, though, so if network traffic
       is involved then the recipient must be able to "roll back" as well.

       This may be a limitation on the types of Node.js applications we can test. 
       If we treat Node.js applications as a "black box" then we need a way to reset the entire system.

    2. Need a way to decide when we have enough pending callbacks to fork and execute.
       Perhaps every time a new callback is received we can fork () and either start executing or NOT start executing.

    3. Review papers on reducing the exponential search space, e.g. partial order reduction.

    4. What happens if one process errors out? I'm guessing it closes stdout and stderr, discomfiting us. Not sure though.
       Need to introduce a fatal error into file_system.js under a particular interleaving.

-------------------------

26 October 2015

4. above: Regarding error'ing out: 
  If one process exits, the other's output is unaffected. It must dup the file descriptors. In retrospect, this makes sense.
  However, if one process relies on the other to log its completion, then we're in trouble.
  Need to wait for completion OR exit. I added a working implementation of this in threadpool.c.

-------------------------

27 October 2015

Discussion with Dr. Lee:
  - Bounded waiting: in time and in number of items in the queue
  - Flip order of only the first X items in the queue
  - Can't change order of JUST execution (cb->work) or JUST cleanup (cb->done),
      because execution maps to the 'request time' and cleanup maps to the 'done time'.
      Two work items could be done in order A, B, and finished in order B, A.
  - Partial order reduction: 
    > Need to read these papers (static [80s], dynamic [00s])
    > Only explore interleavings that actually affect each other
    > Issue here -- how to know if they affect each other in the "black box" approach to callbacks?
      Callbacks X and Y may not be directly related (surface level), but X may trigger callback Z that will affect Y.

  - Bounded reordering: added env variable REORDER_DISTANCE. If 0, do nothing. Otherwise use that reorder distance. Default is 0.
    > Haven't implemented the behavior yet.   
    > One paper used 'forward/backward' rather than spawning K! children. This sounds like a good move.
  - Wrote min_reordering.js that will exit(0) if reordering < X, otherwise exit(1).
    min_reordering.js uses dynamically-generated code so that I don't have to eyeball addresses for correctness;
    each incrementer function has its own "name". Hooray for closures!
  - Developed doubly linked list for a much-cleaner-to-work-with done_list.

-------------------------

28 October 2015

Implemented bounded waiting mechanism based on timeout and REORDER_DISTANCE.
Not quite working -- not quite understanding how to trigger epoll_wait to go back to uv__work_done.
  > output manipulation
  > log function that prefixes pid
  > figure out why epoll_wait isn't being nice

-------------------------

29 October 2015

Still working on the epoll_wait issue.
  - Added 'generation' to mylog for indentation. This should help clarify issues.
  - Idea: what if we spawn ALL children at a level before proceeding on one? This might help
      if the issue is some weird conflict between epoll and fork () (the internet suggests that
      there might be one).

      This leads to memory overhead (a weird combo of BFS and DFS search, but still painful), but 
      if it fixes the problem them that would be swell.

-------------------------

30 October 2015

- Used the combination DFS-BFS search as proposed on 29 Oct. This fixes the problem, hooray!

TODO (by order of priority):
  1. > Need a way to "prove" that it's working properly on a wide variety of inputs.
    Can generate simulated output for min_reordering.js and emit that, then compare.

  2. > Need to determine whether or not child exited 0.

  3. > Need to re-think the notion of a timeout. Because parents wait for children recursively,
    using clock time is a bad idea. 

    Propose instead using "number of loops since I last ran a callback". 
    This would require changing all callback calls to use a uv-common function that sets a 
    flag and then calls the callback. Then in main loop we could bump a counter, also through
    uv-common

  4. > Must avoid "wait forever"-style epoll requests. Never return -1 from the timout function.
    Instead, add a sleep in the main loop -- this effectively simulates "forever", although
    if done naively this would cause "idle" callbacks (and others?) to be executed many more 
    times than might be expected. Might need to take more care with this change.

    3,4: If each loop waits for a second, then "loops since I last ran a callback" has a lower bound of "seconds since I last ran a callback"

-------------------------

8 Nov 2015

- Fixed bug in list_size. Code is now in working order, ready to proceed on verifying correctness tomorrow.

-------------------------

9 Nov 2015

Addressing 1. from 30 October:
  1. Wrote simulator (min_reordering_exploration.pl) to simulate min_reordering.js's output
     when run under a modified Node. Initial results suggest that the simulator output matches
     the real output. Hooray!

  2. Implemented the node-mocks guy's fs.stat example to show the more general power of the 
     libuv reordering approach.

Addressing 2. from 30 October:
  1. Added exit status checking.

  2. It appears that fork() -> pthread_join() is a bad combination. See for example
      developerweb.net/viewtopic.php?id=3633
     Since this is a bit orthogonal, I just don't clean up the thread pool threads.

Outstanding TODOs, carried over from 30 October:
  1. Need to re-think the notion of a timeout. Because parents wait for children recursively,
    using clock time is a bad idea. 

    Propose instead using "number of loops since I last ran a callback". 
    This would require changing all callback calls to use a uv-common function that sets a 
    flag and then calls the callback. Then in main loop we could bump a counter, also through
    uv-common

  2. > Must avoid "wait forever"-style epoll requests. Never return -1 from the timout function.
    Instead, add a sleep in the main loop -- this effectively simulates "forever", although
    if done naively this would cause "idle" callbacks (and others?) to be executed many more 
    times than might be expected. Might need to take more care with this change.

    3,4: If each loop waits for a second, then "loops since I last ran a callback" has a lower bound of "seconds since I last ran a callback"

-------------------------

13 Nov 2015
    
Discussed progress with Dongyoon. Many ideas on paper.

-------------------------

16 Nov 2015

1. Question: How are fs.X and fs.XSync implemented?
  Answer: X is routed into libuv's thread pool work queue for asynchronous handling. XSync () is routed into libuv where it is called synchronously in uv__fs_work. The result is passed back up.

  See the '16 Nov 2015' entry in jamie_node_src_notes for details.

2. I reviewed issues with fork and pthreads. Most notably, in the child only the thread that
called fork() survives. I verified this using the fork_pthread.c test. 

Happily, Node.js only uses two sets of threads: the event loop thread (1 thread) and 
the thread pool threads (X threads).
As a result, getting to a coherent state prior to fork and re-initializing threads after fork
should not be difficult with the judicious use of locks.
  http://www.linuxprogrammingblog.com/threads-and-fork-think-twice-before-using-them
  http://pubs.opengroup.org/onlinepubs/009695399/functions/pthread_atfork.html

This probably also explains the segfault in pthread_join. Note that
both v8 and libuv make use of pthread_join, suggesting that we need
a pthread_at_fork call in both layers.

Question: When and why does v8's Thread::Start get called? I have added prints to show that it happens,
but have not yet tracked down WHY it happens. Next step: gdb

  cdprog
  gdb `which node`

  (gdb) break v8::base::Thread::Start
  Breakpoint 1 at 0xdfa174
  (gdb) c
  The program is not being run.
  (gdb) run file_system.js 
  Starting program: /home/jamie/bin/node file_system.js
  Traceback (most recent call last):
    File "/usr/share/gdb/auto-load/usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.19-gdb.py", line 63, in <module>
      from libstdcxx.v6.printers import register_libstdcxx_printers
  ImportError: No module named 'libstdcxx'
  [Thread debugging using libthread_db enabled]
  Using host libthread_db library "/lib/x86_64-linux-gnu/libthread_db.so.1".

  Breakpoint 1, 0x0000000000dfa174 in v8::base::Thread::Start() ()
  (gdb) bt
  #0  0x0000000000dfa174 in v8::base::Thread::Start() ()
  #1  0x0000000000dcb9bc in v8::platform::WorkerThread::WorkerThread(v8::platform::TaskQueue*) ()
  #2  0x0000000000dc9191 in v8::platform::CreateDefaultPlatform(int) ()
  #3  0x0000000000d68e9e in node::Start(int, char**) ()
  #4  0x00007ffff6becec5 in __libc_start_main (main=0x688310 <main>, argc=2, argv=0x7fffffffe508, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe4f8)
        at libc-start.c:287
  #5  0x000000000068852f in _start ()
  (gdb)

Answer: The threads are launched as part of default_platform, whatever that is.

Conclusion:
pthread_atfork needed to be added at each layer that calls pthreads.
Two places:
  1. V8 uses a pool of threads for its default_platform (whatever that is)
    At the V8 layer, these threads seem to be used to provide "isolates" (independent interpreters?), and it doesn't look like Node makes use of "isolates".
    As a result, no synchronization seemed to be required. I just added a Reinitialize function that would empty and re-create the thread pool.
    
  2. libuv uses a pool of threads for its asynchronous worker pool
    These worker threads are all active, and an uncontrolled fork() could result in inconsistent memory states,
      locked-never-to-be-released mutexes, etc.
    To resolve this, I added a posix semaphore to control the number of active threads. 
    By down'ing this semaphore nthreads times and waiting for all threads to be waiting, I can ensure that
      all worker threads are in a quiesced state at the time of fork ().
      On resume: 
        - the parent post's to the sema
        - the child re-initializes all of the pthread variables and launches nthreads threads

-------------------------

19 Nov 2015

1. Following up on the pthread question, I put printfs into the code to figure out the path by which
the v8 pthreads are assigned work. Work gets into their shared queue via DefaultPlatform::CallOnBackgroundThread

CallOnBackgroundThread seems to be used for compilation and garbage collection tasks
(grep'd the full tree including node and v8). It is called here:

v8/src/optimizing-compile-dispatcher.cc

  OptimizingCompileDispatcher::QueueForOptimization calls:
      V8::GetCurrentPlatform()->CallOnBackgroundThread(
              new CompileTask(isolate_), v8::Platform::kShortRunningTask);


  OptimizingCompileDispatcher::Unblock() calls:
      V8::GetCurrentPlatform()->CallOnBackgroundThread(
              new CompileTask(isolate_), v8::Platform::kShortRunningTask);
  
v8/src/heap/mark-compact.cc
  MarkCompactCollector::StartSweeperThreads calls:
    V8::GetCurrentPlatform()->CallOnBackgroundThread(
          new SweeperTask(heap(), heap()->old_space()),
                v8::Platform::kShortRunningTask);

The sample cluster.js program is sufficient to exercise these threads.
This means that I DEFINITELY need to correctly re-initialize them in a controlled fashion.

Since the WorkerThreads end up waiting on a semaphore in TaskQueue::GetNext, there's already
a built-in waiting mechanism. I added a counter to track the number of waiters, and taught
DefaultPlatform to wait for the queue to empty and all the waiters to be pending prior to
fork'ing. Since the tasks seem to be short (kShortRunningTask, whatever that means!), 
I expect that if the main thread pauses to wait for it to empty, then it will empty fairly quickly.

Implementation appears to have worked. I added a few functions to TaskQueue and now
DefaultPlatform asks it if all of its threads are waiting on it. Since no OTHER threads should be
waiting on the DefaultPlatform's TaskQueue (right?), this seems fine.

2. The Node.js cluster module uses fork(). This makes it unsuitable
to use in my testing environment, since I fork() too. The parent does not know what pid to wait on, and there's
no (?) way to tell the parent that it has a NEW child to wait on (and you'd need to fork the parent itself
to wait on the new child every time the child forks...). Anyway it just seems like a mess.

3. I read the microarchitecture paper on Node.js (I-cache performance issues). It reminded me that as a largely-server-side programming framework, we'll need to think about network inputs as we fork. If we run through once to completion, we can record the network inputs. Afterward we can simulate them? Hmm...

4. I took a peek at how callbacks are invoked:

jamie@lqingrui:~/Desktop/node_project/node/deps/uv/src$ grep -Ri cb\( . | wc -l
141

Threadpool callbacks take 1 or 2 args:
  jamie@lqingrui:~/Desktop/node_project/node/deps/uv/src$ grep -Ri cb\( . | grep threadpool
  ./threadpool.c:  req->work_cb(req);
  ./threadpool.c:  req->after_work_cb(req, err);


More general callbacks take between 1 and 5 args. Here are examples of each:
  ./unix/fs.c:  req->cb(req);
  ./unix/udp.c:      req->send_cb(req, req->status);
  ./unix/getaddrinfo.c:    req->cb(req, req->retcode, req->addrinfo);
  ./unix/getnameinfo.c:    req->getnameinfo_cb(req, req->retcode, host, service);
  ./win/udp.c:      handle->recv_cb(handle, UV_ENOBUFS, &buf, NULL, 0);

19 Nov 2015
Meeting with Dr. Lee.

1. He has a literature list, to which I am welcome to add. For the next meeting, read the Bounded POR and DPOR papers.
2. Discussion about dealing with network traffic. I assert that the major use case for Node.js is as a server to fulfill network requests.  Consequently it's important to be able to handle this.  Probably not an open system (maintaining connections tricky), but can make a closed system (feed the same input). This requires that the client's input does not change based on the server's output. There are clearly cases where this is not true (e.g. two concurrent requests, one to delete a file and one to stat it, and the output will be either "no such file" and "stat values" -- the client will respond differently based on the server's decisions. So let's not discard an open system forever, but for now it's probably necessary to get something working.

3. Proposed architecture:
A. Fork preserves memory state
B. Use file system checkpointing to roll back changes from one run to the next
C. Record network inputs (connections, client data, etc.) and feed that into subsequent runs. Can record the loop number so that we can feed it at the appropriate spot. Modify the epoll mechanism to replay network input at the appropriate spot instead of looking for a live connection. Children close the parent's connections.

4. Question: Node.js uses a lot of external modules. Are these pure JS? Do they interact with libuv? Do they run JS code in the thread pool "work" or just as "done" (e.g. database requests/callbacks)? (single-threaded vs. multi-threaded event driven)

5. Plan to attend ASPLOS Apr. 2-6 '16 in Atlanta.

-----------

15 Dec 2015

1.
I've implemented a unified callback mechanism in an attempt to avoid the issues related to fork.
jamie@suwarna7-Lenovo-K450e:~/Desktop/node_project/node/deps/uv$ git branch
  master
* unified_callback
  uv_reordering

I construct a tree of dependencies between callbacks based on the currently-executing CB.
I'm working on verifying the result. The graphviz library and the xdot viewer for it look useful.

-----------

16 Dec 2015

1. I added a bit more information to a 'struct callback_node'. Now on USR2, the trees are printed 
in graphviz's DOT language for easy visualization. Looks good.

2. The tree looks OK but there's no information about cause and effect. 
In the MUD Node.js project, when a client makes a move it triggers messages to the other clients.
However, the subsequent messages are not children of the client CB.
If I can figure out when the fds of the client CBs are triggered, I can include the info about "who tickled me"
and expand my "cause and effect" picture.

-----------

17 Dec 2015

1. I set the WORK_WORK CBN as the parent of the subsequent WORK_DONE CBN.
2. Now dumping all trees in one meta-tree on receipt of SIGUSR2.
3. Trying to analyze the generated CB tree to determine why there are
   so many WORK_* CBs. It looks like there are some internally-generated
   work items chained together to implement readFile. Perhaps it's 
   open-read-close or something. Presumably a library thing.
4. It seems that responding to a client causes Node (V8?) to generate a TIMER_CB. Not sure why. Presumably a library thing.

-----------

13 Jan 2016

1. Checked on the clients associated with the 6 benchmarks. Lowering the
start window to 5 seconds made this more friendly on MUD, though not sure
if this is going to be bad for others. URL is no good, Dr. Lee is working
on it.
2. Refreshed self on status of code base
3. On SIGINT, dump both visualizations (global callback order and tree view) and exit(0)
4. Unified callback: async calls in init stack
   
   Problem: If the application makes an async call in the initial
   stack (as done in the Micro '15 benchmarks code with an IO
   to an output file), then we didn't have a parent. We assumed async
   calls (to the threadpool ) would always have a parent.
   
   Solution: When submitting work to the threadpool, if there is no
   active CBN then we assume we're in the initial stack. Add a special
   function to return a dummy CBN in this case.
5. Now printing the full tree output to a file in /tmp rather than to stdout. This will allow chaining more readily.

-----------

14 Jan 2016

1. On MUD, verified that the number of callback trees is roughly a function of the number of clients. This is good, since it means that I'm not getting totally bogus-looking numbers.
2. Now printing global callback order to a file, along with client ID and start/duration (in seconds since epoch).
3. Discovered that using the fd associated with a handle as a "client id" will not work. In MUD, for example, handles are opened and closed for each http request -- so 5 requests from 8 clients yields 40 open-and-closed handles. The fds will be re-used. Instead we need to really know the client IP and port. Perhaps this can be stored in the "void* uv_handle_t.data" field of a handle when the handle is created by Node.js? Alternatively, more specific subclasses of a handle DO know this info. For example a uv_udp_t has uv_udp_getsockname() for this purpose. Some investigation required.

-----------

15 Jan 2016

1. Wrote a callback statistics calculator, callback_stats. The callback_stats utility calculates interesting statistics on the "all callbacks" file produced by libuv. This can be used to characterize the behavior of Node.js applications, or of a single application across different inputs.

Sample output is below. Note that it is "bogus" in the sense that client calculations are still incorrect (see NEXT STEPS).
However, the total count of callbacks and the per-type counts are accurate.

jamie@suwarna7-Lenovo-K450e:~/bin$ ./callback_stats /tmp/callback_global_order_1452867867_24003.txt
Results from /tmp/callback_global_order_1452867867_24003.txt
------------------------------------------------------------------------------------
Statistic                                                       Value               
------------------------------------------------------------------------------------
Instances of type UV_READ_CB                                    80                  
Client -1: Instances of type UV_ASYNC_CB                        3                   
Instances of type UV__WORK_WORK                                 3                   
Client 12: Instances of type UV_CONNECTION_CB                   40                  
Client 11: Instances of type UV_READ_CB                         80                  
Instances of type UV__IO_CB                                     206                 
Client -1: Instances of type UV_TIMER_CB                        1                   
Client -1: Instances of type UV__WORK_DONE                      3                   
Instances of type UV__WORK_DONE                                 3                   
Client -1: Instances of type UV_FS_CB                           3                   
Instances of type UV__ASYNC_CB                                  3                   
Client -1: Instances of type UV__WORK_WORK                      3                   
Callbacks with 0 children                                       327                 
Instances of type UV_CONNECTION_CB                              40                  
Client 12: number of callbacks                                  40                  
Instances of type UV_ALLOC_CB                                   80                  
Client 11: number of callbacks                                  160                 
Instances of type UV_TIMER_CB                                   1                   
Client -1: Instances of type UV_CLOSE_CB                        40                  
Instances of type UV_FS_CB                                      3                   
Client -1: Instances of type UV__IO_CB                          206                 
Client -1: Instances of type UV__ASYNC_CB                       3                   
Instances of type UV_CLOSE_CB                                   40                  
Client 11: Instances of type UV_ALLOC_CB                        80                  
Client -1: number of callbacks                                  262                 
Callbacks with 1 children                                       55                  
Callbacks with 2 children                                       80                  
Number of callbacks                                             462                 
Instances of type UV_ASYNC_CB                                   3                   
Number of clients                                               3     

-----------

18 Jan 2016

1. Made lists thread-safe internally.
2. Developed a map class for future use. It does not appear to be immediately relevant but "you never know". I thought it would be useful but then I came up with a better approach.
3. Determined that with a tcp handle I can identify the IP and (port? socket?) of the client. This information can be inherited by children. See notes from 1/18.

-----------

19 Jan 2016

1. Debug map class. Debug list class. Add unit tests for each. Commit changes.
2. Make an offline backup of progress so far.
3. Work on client_id. Experiment with lets-chat to see what another set of trees looks like.
  .dot files:
  ls /tmp/*clients.dot
    /tmp/lets-chat_clients.dot  /tmp/lets-chat_manual_clients.dot  /tmp/mud_clients.dot

-----------

20 Jan 2016

1. Implement two client ID maps. The first map is <internal client ID -> hash(sockaddr)>, and the second is <hash(sockaddr) -> "struct sockaddr">. In invoke_callback, if a stream is available then we can (unsafely!) obtain the struct sockaddr associated with it. From this we can determine the client ID. The client ID is based on a hash of the client's IP address and port number.

  On the utility of this approach: 
    mud: The http request embeds the client ID. Each move triggers a new http request (connection) that uses a new port, which causes each user's moves to manifest as a different client. This seems like an inefficient implementation to me. However, the point here is that the application can do "whatever it wants", and this may compromise attempts to identify clients at the framework level.
    lets-chat: There are some unknown client IDs going on here, but the implementation overall has all chat traffic from a user routed through the same connection. I have not tried clicking other buttons. For example, if I start a new chat room, does libuv still know it's me, or does it think I'm a new user?

  A general solution here would be to have one IP per client, and to reduce the client ID to a computation solely based on IP address. This seems like a workable approach, though at the moment it's not good because of the way clients work.
  Another approach would be application-level annotation. This would be undesirable.

2. In progress: once one node in a tree knows its client ID, the entire tree is marked as belonging to that client.

3. Enhance callback_stats to extract and print the "complete schedule" -- the order in which client-driven callbacks (i.e. callbacks that originated from a client).

-----------

21 Jan 2016

1. Think carefully about replayability and tree structures.
2. First draft of correct timer inheritance.

-----------

25 Jan 2016

1. Correct timer inheritance.
2. Color nodes based on client ID.
3. Improved detection of client ID.
4. Add new API uv_init_stack_done so that libuv can distinguish between CBs registered/executed
   in the initial stack vs. those triggered by client input.

-----------

26 Jan 2016

1. Debugging/committing of the 25 Jan. changes.
2. Fix misc compilation warnings.
3. Capture check, prepare, and idle callback registration and inheritance. See src/unix/loop-watcher.c
4. Microsecond granularity for a CBN's "start" field. Otherwise the field was useless.

-----------

27 Jan 2016

I proofread Dr. Lee's TxRace paper for ASPLOS '16 from noon until 11:30 PM.

-----------

28 Jan 2016

1. Mark begin and end of init stack and uv_run. 
Next step: Create a map of callback_address -> source,
where source is one of {NODE_INTERNAL, USER}. Instrumenting the 
callback registration points combined with calls to uv__init_stack_active
and uv__uv_run_active should suffice. 
I am assuming that only one Node implementation thread might interact with uv at a time. We'll see how true that is. If not true, the recourse would be
a per-thread status variable. Easy enough -- note the tid of all threads
that register callbacks.

-----------

29 Jan 2016

No work today.

-----------

1 Feb 2016

1. Complete implementation of origin tracking. This lets me identify CBNs originating from internal Node.js and ibuv code. I can now identify everything but leftover "UV__IO_CBs" that emerge during uv__run_pending. So far I have not seen these register any CBs, so I think this is OK. However, I regret not knowing which CB caused their registration. Perhaps this could be tracked later.
2. Refactor invoke_callback a bit to reduce bloat. Make use of helper functions.
3. Use clock_gettime for a monotonically increasing "start" value. Also use this for the duration calculations. 

-----------

2 Feb 2016

1. Refactor invoke_callback a bit more so that the giant switch is in its own function. Makes invoke_callback more readable.
2. Play on systemG.
3. bitbucket/noderacer: vanilla mode (no Jalangi).
4. Think about uv__run_pending lineage determination.

-----------

3 Feb 2016

1. Play with systemG. Get passwordless ssh, pdsh, bitbucket, etc.
2. Think about uv__run_pending lineage determination.
3. bitbucket/noderacer: vanilla mode (no Jalangi), pick your server IP and port

-----------

4 Feb 2016

1. Implement uv__run_pending lineage determination. Verify that things "look OK" on MUD and lets-chat.
2. Add an assert that threadpool threads only run "expected" CBs (UV__WORK_WORK and UV_WORK_CB).
3. Bugfix: Set init_stack_CBN as the current CBN during the initial stack so that all CBs descend appropriately during the initial stack. 
4a. Implement per-thread current CBN. Still need to propagate this change to affected portions of uv-common.c

-----------

5 Feb 2016

1. Re-work of lineage determination for threadpool. Realized I was doing this a bit incorrectly. Alas. Should explain the unknown+oddly-located WORK CBs in lets-chat. See notes in jamie_libuv_src_notes for details.
  Currently debugging my solution. Blowing up in lets-chat with a CB pointing to a CBN?? 


-----------

6-9 Feb 2016

1. Change tree design a bit. I decided that changing the parentage of a node was unwise at best. This resulted in an after-the-fact change to the schedule, which would make it impossible to "retrace my steps" on a subsequent run given a schedule. Instead, I'm now maintaining dual physical/logical parentage information. The physical tree is a faithful reproduction of the actual order in which callbacks passed through libuv during a run. The logical tree provides extra edges where relationships are not provided by the physical tree. For example, the relationship of repeating timers to themselves, or the relationships of UV__WORK_WORK to UV__WORK_DONE, is not visible from a physical tree. However, tracking it is important, and the logical tree gives us that. In essence, I'm now keeping ALL the information, instead of throwing some of it away.
TODO Discuss thoughts on inheritance and coloring

-----------

9 Feb 2016

1. Eliminate all tree inheritance and coloring. I believe that inheritance coloring should be
  added back in, and should cross logical barriers. However, I think tree coloring is unsafe.
  Consider for example the effect on httpserver_batch.js.

2. cdprog: Developed httpserver_{straightforward, batch, background} to demonstrate the callback tree
  associated with each. Use ./http_client.js to exercise them.

3. Change the shape of CB nodes based on whether they corresponded to client input, output, or neither. 
  Note: READ callbacks are invoked when data is read from a stream (client). These CBs will begin the response to the client's input. WRITE callbacks are invoked after data is written to a stream (client). These CBs come at the END of the output. This means that delaying the execution of a WRITE callback does not actually delay the output, just the notification of the output. If the output is coming indirectly (e.g. httpserver_batch), grabbing the WRITE callback won't tell us this.
  This suggests that marking the callback in uv_write2 as "output to client X" would be a useful step.

4. Write a program that uses async.parallel to see what it looks like at the libuv level

5. I've decided that determining which CBs are client code is probably not helpful (even if I could figure out how to do it -- via origin?). UV__WORK_WORK is not client code, but it includes the Node.js implementation of FS IO. Essentially UV__WORK_WORK *is* client code, but it's not registered as such. Rather, inheritance tells us this. But inheritance tells us this anyway.

6. Working on an SQLite DB program to see how SQLite APIs are transformed to libuv CBs.

-----------

10 Feb 2016

1. After a lot of wrestling, I've managed to convince node to install a development version of sqlite3 for my use. The issue was that unlike other modules I've installed (e.g. async), sqlite3 uses C++ bindings. The fix was to change the /usr/bin versions of nodejs to point to my development copy, and then to run 'npm install sqlite3' as usual. Strangely enough, I thought I'd done that already. I also installed node-gyp, not sure if that had an effect. 

There was advice online about using 'npm install --nodedir /path/to/devel/copy sqlite3' but this did not work. Perhaps I was using it wrong. The npm documentation feels wanting.

Now that I've got a working version of sqlite3 installed, it appears that my development version of node segfaults when running 'node sqlite_simple.js'. HOWEVER if I 'git branch master', the code runs fine. Interesting. My guess is that it's somehow related to using the C++ binding, as I haven't tried that before. 

Next step: attempt a C++ binding tutorial and see if that also segfaults on my version.

-----------

11 Feb 2016

1. I found the issue. The node-sqlite3 documentation wasn't quite up to snuff. Here's the secret sauce:

npm install --build-from-source --verbose --sqlite=/usr/local --nodedir=/home/jamie3/Desktop/node_project/node

The key was to add --build-from-source, which makes use of the src directory. Without it, any
local changes in src are just ignored.
I actually think the --nodedir is unnecessary since I've run 'make install' and installed the development
version on my system.
I made use of freenode's #node-dev IRC channel through ChatZilla to double-check that I was going through
the right motions (except for the --build-from-source). bradleymeck and evanlucas were helpful.

2. The trees arising from sqlite_simple.js are interesting. I'm going to make a collection of trees to walk through with Dongyoon today. I'd like to come prepared with a "demo" like the one I did today (see the weekly_meetings folder). It was really helpful for discussion.

3. Meeting with Dongyoon.
  We are 6 weeks out from the OOPSLA deadline. We'll see how development goes, but here are some tentative milestones:

-----------

12 Feb 2016

1. I've finally figured out how to add debug breakpoints to node.cc. In $NODE, run './configure --debug' and then 'make; make install'. This produces binaries called 'node' in $NODE/out/Release and $NODE/out/Debug. Use gdb on the Debug version. Then in gdb run 'break node::StartNodeInstance' or 'break node.cc:3972'. There's a symlink in ~/bin called node-debug for ease of use.

2. Explored promises for the ghost engine. I haven't finished learning about these but I'm making progress. An initial example is in /home/jamie3/Desktop/node_project/node/jamie/simple_node_programs/promises/promise_rsvp.js. See squid.ore email for reading material. My conclusion is that promises can be used asynchronously in a ``baton'' fashion, and that in the Ghost example they are used in that fashion. If *not* used in this fashion, they're basically being used incorrectly. Their purpose is to order asynchronous events; if each promise includes only synchronous code then there's no reason not to write it synchronously. Example:

if(x = 1).then(y = 2).then... 
  should be
x = 1
y = 2
...

-----------

15 Feb 2016

1. Resume client inheritance
2. Research timeline software

-----------

16 Feb 2016
17 Feb 2016

I spent these days learning about TheTimelineProj and developing a library to automatically generate XML for examination with it.
A default view was not useful because the range in time between events was too large.
Steps taken to address this:
  - normalization
  - era detection

-----------

18 Feb 2016

1. Complete working version of timeline interface. Still needs a true CLI. Use it to produce visualizations for identical executions against the three httpserver instances. timeline_demo is on my laptop. I've checked the timeline code into the node git repo on woody.

----------

19 Feb 2016

I spent today thinking about deterministic replay.

I wondered whether scheduling callbacks in pure JS seemed feasible.

I think to do so would require instrumenting the source code of any module that "rolls its own" libuv interactions. I don't like this idea, and it also introduces overhead (not as much as Jalangi, I think, but still non-trivial).

Scheduling them instead in libuv seems wiser. I think by disabling the V8 garbage collector we can safely retain references to callbacks forever, and only invoke them when we want to. In libuv we also have full control over the threadpool without having to worry about multiple components (JS and libuv) and communication between them.


----------

21 Feb 2016
22 Feb 2016

1. I attempted a first draft at a callback deferral mechanism. This version uses a scheduler thread. invoke_callback routes callbacks into a scheduler list for eventual execution. The scheduler thread picks callbacks off of the list and executes them.

Unfortunately this leads us to two related concerns. First, libuv thinks of itself as the bottom of the stack. Once a callback is invoked, libuv believes that it is done (and might proceed to free memory, dismantle the handle with which it's associated, etc.). If instead I've squirreled it away to execute later, that's a segfault waiting to happen. Second, once libuv thinks it is done, it may eventually (?) communicate to Node.js that the callback is no longer needed, and the V8 GC will scoop it up.

I do not think that pursuing this approach is worthwhile.

2. I pondered an alternative mechanism, still libuv-based. 
Suppose that in linux-core:uv__io_poll:
  1. Each pending fd was associated with exactly one "relevant" user-defined callback (e.g. ignore UV_ALLOC_CB and focus on UV_READ_CB) 
  2. We could determine the user-defined callback with which it was associated in a uniquely-identifiable way
Then we could defer invoking the UV__IO_CB whose turn has not come up yet, and have determinism without needing to worry about the V8 GC or the Node-C++ implementation.

Now, examining the timelines I've generated so far, I see that a single UV__IO_CB can actually wrap multiple user callbacks.
This occurs due to indirection in libuv. I think the indirection is to reduce the number of fds on which to epoll_wait. For example, instead of one fd for each async event, we have a single "async fd" and arm it when IO is observed on any of the async events. The specific fd(s) triggered is determined by the async handler. However, we can fudge the "ready" events to pinpoint the precise CBs we want to invoke out of the set of ready fds.

This approach seems much more promising. I'll have to think about the details further after lunch.
See jamie_libuv_src_notes for details.

----------

23 Feb 2016

1. Lots of pondering in jamie_libuv_src_notes regarding the feasibility of my second approach.
2. I think I've reached a good point with my second approach. It'll require a decent amount of mucking about in libuv, but it seems sound.
   I think I should try to implement a first draft using something easy -- timers.
   Based on sample_prog timers.js, the only CBs that are triggered in a timer-only workload are UV__IO_CB and UV_TIMER_CB. This is as expected.
   A quick count suggests that timer callbacks are coalesced by Node. A poke around node/lib/timers.js and IRC suggests that:
    1. Timers are indeed coalesced
    2. These are the only callbacks coalesced above libuv

    Commit 7d2b9009 disables timer coalescing, at least roughly.

----------

24-29 Feb 2016

1. I've implemented the new tracking scheme (Logical Callback Nodes - LCBNs) alongside the original CBN code.
   I figured why delete the CBN code unless I was sure I wouldn't need it?
2. I've done refactoring and improvements (e.g. unified-callback-enums.[ch]).
3. I've moved the graphviz stuff into Python. All I need is a printout of the LCBNs and I can produce a tree. This is better; with a constant input file I can tweak the display. More modular.

----------

29 Feb 2016

1. All threadpool code is now routed through uv_queue_work. This required adding wrappers in fs.c, getaddrinfo.c, and getnameinfo.c.
2. Begin work on dependencies (nodes other than the registering parent that precede a node). Plot these in Python using dashed lines.

---------

2 Mar 2016

1. LCBN order info: differentiate between registration order and execution order.
2. Don't track the dummy CB from uv_try_write. If it gets executed we're doomed anyway.

---------

3 Mar 2016

1. LCBN tracking: WORK -> FS_WORK -> AFTER_WORK -> FS_CB, etc.
    Note that this is a bit misleading; unlike all(?) other LCBNs, WORK/FS_WORK and AFTER_WORK/FS_CB (etc.)
    cannot be executed independently. They are a unit. Scheduling logic will need to take this into
    consideration.
2. Record registration time for comparison with execution start.
   If the gap between registration time and execution start grows over time, this
   implies that the threadpool is overloaded and should be re-sized. Could be used
   to improve Node performance. libuv devs have been thinking about this since ~2014:
     https://github.com/libuv/leps/pull/4
3. callbackVis: optional removal of un-executed nodes
4. CLI work:
    - Refactor TimelineCLI to use the CallbackNode class
    - One-shot CBN file -> timeline and tree

Next up:
  LCBN extraction: identify the LCBNs associated with run_timers/run_idle/etc. as well as with the fds found in uv__io_poll
    > Fill in APIs like uv__ready_timers. Do depth rather than breadth, agile. Start with timers, then check/idle/prepare, then...

---------

4 Mar 2016

1. Mock up a scheduler API and implementation. Not yet complete. I think 1 more public APIC and 1-2 private APIs are needed; see TODOs in scheduler.[hc].
2. Rewrite uv__run_timers to use the scheduler APIs.

---------

7 Mar 2016

1. scheduler_init: REPLAY path (read from file; load desired_schedule)

---------

8 Mar 2016

1. scheduler: implement record path

---------

9 Mar 2016

1. scheduler: work through some bugs
2. schedule specification (record/replay and file) via env variables:
  UV_SCHEDULE_MODE
  UV_SCHEDULE_FILE

During testing I discovered that identifying nodes by <tree, level, entry relative to parent> won't work.

      O
     / \
    O   O
    |   |
    O   O  <-- these are both <0,2,0>, how to distinguish? 

The issue is that from <0,2,0> we don't know who the parent is.
Instead I think I must use <parent, relative child number> as the unique ID. This is more obviously correct.

- Demonstrate timer scheduling.

-----------

STEPS:
- Assess flippability calculations I can do "now". Am I missing anything?
- callback_stats: extract the "root schedule". This is order in which *new* external inputs were received from clients, different from the "complete schedule" interplay between the callback trees descending from multiple concurrent external inputs.

- An interesting comparison: "random testing" inspired by the empirical study. At the JS level, insert rand_sleep into any asynchronous method. See how much variation in the schedule arises.


  Feb. 18: 
    X inheritance coloring, favoring logical parentage - 1/2 day
    X explore the library used in Arun's example that offers X.thenY.thenZ semantics. I believe it's called promise.js. See whether it makes a "big blob" or uses a baton approach. Batons mean that we can find Arun's race condition; big blob means that it's truly a "threadpool race" and will only manifest if the two blobs are run concurrently. - 1/2 day
    X visualization of execution order, including overlapping CBs - 1/2 day
    o initial definition(s) and extraction(s) of schedule - 1 day
    o measure variations in schedule(s) for simple and more complex apps - 1 day
    o explore identification of "fat functions" a la word-finder. This is part of the analysis stage. 1/2 day

  Feb. 25:
    - Add a scheduled execution mode (i.e. follow the provided schedule as closely as possible). 3 days.
        May want per-client CB tree visualization?
        The tricky part is that the first step off of the original schedule might radically change the rest of the schedule -- e.g. httpserver_batch.js. Go back to record mode? Potentially too expensive.
    - Try naive schedule exploration of simple apps. 2 days.
    - This is probably too aggressive to get CORRECT in one week, but we'll see. I won't propose schedules beyond that because I'm not confident about the size of this milestone.
  
  Mar. 3:
    - ?
